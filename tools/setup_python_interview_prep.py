#!/usr/bin/env python3
"""
Setup Python Analytics Interview Prep - With Modes
Generated: 2025-10-03 02:00:34
Configuration: with Wing Pro

USAGE:
    # Step 1: Set up local repository only (default)
    python setup_python_interview_prep.py --mode local
    
    # Step 2: After testing locally, build Docker
    python setup_python_interview_prep.py --mode docker
    
    # Or do both at once
    python setup_python_interview_prep.py --mode all

MODES:
    local  - Set up repository only (test locally first)
    docker - Build Docker only (requires local repo exists)
    all    - Set up repository AND build Docker

Creates: ./python-analytics-interview-prep/
"""

import os
import sys
import json
import base64
import shutil
import argparse
import subprocess
from pathlib import Path

# Repository name
REPO_NAME = "python-analytics-interview-prep"

# Wing included in this build
WING_INCLUDED = True

# Target structure
TARGET_STRUCTURE = {
  "src": [
    "exercises.py",
    "patterns_and_gotchas.py"
  ],
  "docs": [
    "README.md",
    "SETUP_GUIDE.md",
    "LEARNING_GUIDE.md",
    "PLATFORM_ARCHITECTURE.md",
    "course_with_schedule.md",
    "talking_points.md",
    "quick_reference.md"
  ],
  "data": [
    "flashcards_complete.xlsx"
  ],
  "docker": [
    "Dockerfile",
    "requirements.txt"
  ],
  "practice_work": [],
  "mock_interviews": [],
  "notes": []
}

# Preserved directories
PRESERVED_DIRS = ['practice_work', 'mock_interviews', 'notes', '.git', 'venv', '__pycache__']

# Text artifacts (embedded as JSON for safety)
TEXT_ARTIFACTS = {
  "src/exercises.py": "#!/usr/bin/env python3\n\"\"\"\nPython Analytics Engineering Interview Prep - Exercises\n60 exercises (10 per module) with solutions and explanations\nProgressive difficulty: Easy \u2192 Medium \u2192 Hard \u2192 Debug \u2192 Interview\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any, Tuple\nimport json\n\n# ============================================================================\n# MODULE 1: DATA STRUCTURES & OPERATIONS\n# ============================================================================\n\ndef exercise_1_1():\n    \"\"\"\n    EASY: Create a list of numbers 1-10 and return only even numbers.\n    Time: 5 minutes\n    \"\"\"\n    # Problem\n    print(\"Create a list of numbers 1-10 and return only even numbers.\")\n    \n    # Your solution here:\n    # numbers = ???\n    # evens = ???\n    \n    # Solution\n    numbers = list(range(1, 11))  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    \n    # Method 1: List comprehension (best)\n    evens = [n for n in numbers if n % 2 == 0]\n    \n    # Method 2: Filter\n    evens_filter = list(filter(lambda x: x % 2 == 0, numbers))\n    \n    # Method 3: Loop (verbose but clear)\n    evens_loop = []\n    for n in numbers:\n        if n % 2 == 0:\n            evens_loop.append(n)\n    \n    print(f\"Result: {evens}\")  # [2, 4, 6, 8, 10]\n    return evens\n\ndef exercise_1_2():\n    \"\"\"\n    MEDIUM: Given a list with duplicates [1, 2, 2, 3, 3, 3, 4], \n    return unique values in original order.\n    Time: 8 minutes\n    \"\"\"\n    # Problem\n    data = [1, 2, 2, 3, 3, 3, 4]\n    print(f\"Remove duplicates from {data}, preserving order\")\n    \n    # Solution\n    # Method 1: Dict.fromkeys() trick (Python 3.7+ preserves order)\n    unique = list(dict.fromkeys(data))\n    \n    # Method 2: Manual tracking (more explicit)\n    seen = set()\n    unique_manual = []\n    for item in data:\n        if item not in seen:\n            seen.add(item)\n            unique_manual.append(item)\n    \n    # Method 3: Using pandas (overkill but works)\n    unique_pandas = pd.Series(data).drop_duplicates().tolist()\n    \n    print(f\"Result: {unique}\")  # [1, 2, 3, 4]\n    \n    # Explanation: Order matters! set(data) would lose order.\n    return unique\n\ndef exercise_1_3():\n    \"\"\"\n    HARD: Merge two dictionaries, but if keys overlap, keep the higher value.\n    Time: 10 minutes\n    \"\"\"\n    # Problem\n    dict1 = {'a': 10, 'b': 20, 'c': 30}\n    dict2 = {'b': 25, 'c': 15, 'd': 40}\n    print(f\"Merge {dict1} and {dict2}, keeping higher values\")\n    \n    # Solution\n    # Method 1: Iterate and compare\n    merged = dict1.copy()\n    for key, value in dict2.items():\n        if key not in merged or value > merged[key]:\n            merged[key] = value\n    \n    # Method 2: Using dict comprehension with union\n    all_keys = set(dict1.keys()) | set(dict2.keys())\n    merged_comp = {\n        key: max(dict1.get(key, float('-inf')), \n                dict2.get(key, float('-inf')))\n        for key in all_keys\n    }\n    \n    print(f\"Result: {merged}\")  # {'a': 10, 'b': 25, 'c': 30, 'd': 40}\n    \n    # Explanation: b=25 (not 20), c=30 (not 15), d=40 (new)\n    return merged\n\ndef exercise_1_4():\n    \"\"\"\n    DEBUG: Fix this code that should remove duplicates from a list.\n    Time: 7 minutes\n    \"\"\"\n    # Broken code\n    def remove_duplicates_broken(items):\n        # BUG: Modifying list while iterating\n        for i, item in enumerate(items):\n            if items.count(item) > 1:\n                items.remove(item)\n        return items\n    \n    # Problem demonstration\n    test_data = [1, 2, 2, 3, 3, 3, 4]\n    # result = remove_duplicates_broken(test_data.copy())\n    # This would skip elements!\n    \n    # Fixed solution\n    def remove_duplicates_fixed(items):\n        # Option 1: Use set (loses order)\n        # return list(set(items))\n        \n        # Option 2: Preserve order\n        seen = set()\n        result = []\n        for item in items:\n            if item not in seen:\n                seen.add(item)\n                result.append(item)\n        return result\n    \n    result = remove_duplicates_fixed(test_data)\n    print(f\"Fixed result: {result}\")  # [1, 2, 3, 4]\n    \n    # Explanation: Never modify a list while iterating over it!\n    return result\n\ndef exercise_1_5():\n    \"\"\"\n    INTERVIEW: Count frequency of words in a sentence using a dictionary.\n    Handle case-insensitive matching and punctuation.\n    Time: 12 minutes\n    \"\"\"\n    # Problem\n    sentence = \"The quick brown fox jumps over the lazy dog. The fox is quick!\"\n    print(f\"Count word frequency in: {sentence}\")\n    \n    # Solution\n    import string\n    \n    # Clean and split\n    # Remove punctuation and convert to lowercase\n    cleaned = sentence.lower().translate(\n        str.maketrans('', '', string.punctuation)\n    )\n    words = cleaned.split()\n    \n    # Method 1: Manual dictionary building\n    frequency = {}\n    for word in words:\n        frequency[word] = frequency.get(word, 0) + 1\n    \n    # Method 2: Using Counter (best practice)\n    from collections import Counter\n    frequency_counter = Counter(words)\n    \n    # Method 3: Using defaultdict\n    from collections import defaultdict\n    frequency_default = defaultdict(int)\n    for word in words:\n        frequency_default[word] += 1\n    \n    # Sort by frequency (common interview addition)\n    sorted_freq = dict(sorted(frequency.items(), \n                             key=lambda x: x[1], \n                             reverse=True))\n    \n    print(f\"Word frequencies: {sorted_freq}\")\n    # {'the': 2, 'quick': 2, 'fox': 2, ...}\n    \n    return sorted_freq\n\n# ============================================================================\n# MODULE 2: LIST COMPREHENSIONS & STRING OPERATIONS\n# ============================================================================\n\ndef exercise_2_1():\n    \"\"\"\n    EASY: Use list comprehension to get squares of only positive numbers \n    from [-2, -1, 0, 1, 2, 3].\n    Time: 5 minutes\n    \"\"\"\n    # Problem\n    numbers = [-2, -1, 0, 1, 2, 3]\n    print(f\"Get squares of positive numbers from {numbers}\")\n    \n    # Solution\n    # Note: 0 is not positive!\n    squares = [x**2 for x in numbers if x > 0]\n    \n    # Common mistake (includes 0)\n    # wrong = [x**2 for x in numbers if x >= 0]\n    \n    print(f\"Result: {squares}\")  # [1, 4, 9]\n    \n    # Alternative: Include transformation in condition\n    squares_alt = [x**2 for x in numbers if x > 0]\n    \n    return squares\n\ndef exercise_2_2():\n    \"\"\"\n    MEDIUM: Given list of names, create dictionary with name as key \n    and name length as value.\n    Time: 7 minutes\n    \"\"\"\n    # Problem\n    names = [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\"]\n    print(f\"Create length dictionary from {names}\")\n    \n    # Solution\n    # Dictionary comprehension\n    name_lengths = {name: len(name) for name in names}\n    \n    # Alternative: Using zip\n    name_lengths_zip = dict(zip(names, map(len, names)))\n    \n    # Filter variant: Only names longer than 3 chars\n    long_names = {name: len(name) for name in names if len(name) > 3}\n    \n    print(f\"Result: {name_lengths}\")\n    # {'Alice': 5, 'Bob': 3, 'Charlie': 7, 'Dave': 4, 'Eve': 3}\n    \n    return name_lengths\n\ndef exercise_2_3():\n    \"\"\"\n    HARD: Flatten this nested structure [1, [2, 3], [4, [5, 6]], 7] \n    to [1, 2, 3, 4, 5, 6, 7].\n    Time: 12 minutes\n    \"\"\"\n    # Problem\n    nested = [1, [2, 3], [4, [5, 6]], 7]\n    print(f\"Flatten nested structure: {nested}\")\n    \n    # Solution\n    def flatten(lst):\n        \"\"\"Recursively flatten nested list\"\"\"\n        result = []\n        for item in lst:\n            if isinstance(item, list):\n                result.extend(flatten(item))\n            else:\n                result.append(item)\n        return result\n    \n    flat = flatten(nested)\n    \n    # Alternative: Using generator for memory efficiency\n    def flatten_gen(lst):\n        for item in lst:\n            if isinstance(item, list):\n                yield from flatten_gen(item)\n            else:\n                yield item\n    \n    flat_gen = list(flatten_gen(nested))\n    \n    print(f\"Result: {flat}\")  # [1, 2, 3, 4, 5, 6, 7]\n    \n    # Note: For single-level nesting, use:\n    # flat_simple = [item for sublist in nested for item in sublist]\n    \n    return flat\n\ndef exercise_2_4():\n    \"\"\"\n    DEBUG: Fix the comprehension that should get words longer than 3 characters.\n    Time: 5 minutes\n    \"\"\"\n    # Broken code\n    sentence = \"The quick brown fox jumps\"\n    # wrong = [word for word in sentence if len(word) > 3]\n    # BUG: Iterates over characters, not words!\n    \n    # Fixed solution\n    words_correct = [word for word in sentence.split() if len(word) > 3]\n    \n    # Additional: Handle punctuation\n    import string\n    sentence_punct = \"The quick, brown fox jumps!\"\n    words_clean = [\n        word.strip(string.punctuation) \n        for word in sentence_punct.split() \n        if len(word.strip(string.punctuation)) > 3\n    ]\n    \n    print(f\"Result: {words_correct}\")  # ['quick', 'brown', 'jumps']\n    \n    return words_correct\n\ndef exercise_2_5():\n    \"\"\"\n    INTERVIEW: Clean email addresses - lowercase, remove spaces, validate format.\n    Return list of (email, is_valid) tuples.\n    Time: 10 minutes\n    \"\"\"\n    # Problem\n    emails = [\n        \"  Alice@Example.COM  \",\n        \"bob@domain\",\n        \"charlie@email.co.uk\",\n        \"not.an.email\",\n        \"dave@\",\n        \"eve@company.org\"\n    ]\n    print(f\"Clean and validate emails: {emails}\")\n    \n    # Solution\n    def clean_and_validate(email_list):\n        result = []\n        \n        for email in email_list:\n            # Clean\n            cleaned = email.strip().lower()\n            \n            # Validate (simple check)\n            is_valid = (\n                '@' in cleaned and \n                '.' in cleaned.split('@')[1] if '@' in cleaned else False\n            )\n            \n            result.append((cleaned, is_valid))\n        \n        return result\n    \n    # Using list comprehension\n    def validate_email(email):\n        email = email.strip().lower()\n        parts = email.split('@')\n        return (\n            len(parts) == 2 and \n            len(parts[0]) > 0 and \n            '.' in parts[1]\n        )\n    \n    cleaned_emails = [\n        (email.strip().lower(), validate_email(email))\n        for email in emails\n    ]\n    \n    print(f\"Result: {cleaned_emails}\")\n    # [('alice@example.com', True), ('bob@domain', False), ...]\n    \n    return cleaned_emails\n\n# ============================================================================\n# MODULE 3: FUNCTIONS & LAMBDA\n# ============================================================================\n\ndef exercise_3_1():\n    \"\"\"\n    EASY: Write a function with default parameters that formats \n    a number as currency.\n    Time: 5 minutes\n    \"\"\"\n    # Solution\n    def format_currency(amount, symbol='$', decimals=2, thousands_sep=','):\n        \"\"\"\n        Format number as currency with customizable options.\n        \n        Args:\n            amount: Numeric value\n            symbol: Currency symbol (default: $)\n            decimals: Decimal places (default: 2)\n            thousands_sep: Thousands separator (default: ,)\n        \"\"\"\n        # Format with thousands separator and decimals\n        formatted = f\"{amount:,.{decimals}f}\"\n        \n        # Add currency symbol\n        return f\"{symbol}{formatted}\"\n    \n    # Test\n    print(format_currency(1234567.89))  # $1,234,567.89\n    print(format_currency(1234567.89, symbol='\u20ac'))  # \u20ac1,234,567.89\n    print(format_currency(1234567.89, decimals=0))  # $1,234,568\n    \n    return format_currency\n\ndef exercise_3_2():\n    \"\"\"\n    MEDIUM: Use lambda with sorted() to sort list of tuples by second element.\n    Time: 7 minutes\n    \"\"\"\n    # Problem\n    data = [\n        ('Alice', 85),\n        ('Bob', 92),\n        ('Charlie', 78),\n        ('Dave', 92),\n        ('Eve', 88)\n    ]\n    print(f\"Sort by score: {data}\")\n    \n    # Solution\n    # Sort by second element (score)\n    sorted_by_score = sorted(data, key=lambda x: x[1])\n    \n    # Sort descending\n    sorted_desc = sorted(data, key=lambda x: x[1], reverse=True)\n    \n    # Sort by score (desc), then name (asc) for ties\n    sorted_complex = sorted(data, key=lambda x: (-x[1], x[0]))\n    \n    print(f\"Sorted ascending: {sorted_by_score}\")\n    print(f\"Sorted descending: {sorted_desc}\")\n    print(f\"Score desc, name asc: {sorted_complex}\")\n    \n    return sorted_complex\n\ndef exercise_3_3():\n    \"\"\"\n    HARD: Create a function that returns a function (closure) \n    for custom filtering.\n    Time: 10 minutes\n    \"\"\"\n    # Solution\n    def create_filter(min_value=None, max_value=None, exclude_values=None):\n        \"\"\"\n        Create a custom filter function with configured parameters.\n        \n        Returns a function that filters based on the configured criteria.\n        \"\"\"\n        exclude_set = set(exclude_values) if exclude_values else set()\n        \n        def filter_func(value):\n            # Check exclusions first\n            if value in exclude_set:\n                return False\n            \n            # Check min boundary\n            if min_value is not None and value < min_value:\n                return False\n            \n            # Check max boundary\n            if max_value is not None and value > max_value:\n                return False\n            \n            return True\n        \n        # Return the configured filter\n        return filter_func\n    \n    # Usage examples\n    # Create filter for values between 10-100, excluding 50\n    my_filter = create_filter(min_value=10, max_value=100, exclude_values=[50])\n    \n    data = [5, 10, 25, 50, 75, 100, 150]\n    filtered = list(filter(my_filter, data))\n    print(f\"Filtered result: {filtered}\")  # [10, 25, 75, 100]\n    \n    # Create another filter instance\n    positive_filter = create_filter(min_value=0)\n    positive_nums = list(filter(positive_filter, [-5, -1, 0, 1, 5]))\n    print(f\"Positive only: {positive_nums}\")  # [0, 1, 5]\n    \n    return create_filter\n\ndef exercise_3_4():\n    \"\"\"\n    DEBUG: Fix the mutable default argument bug in the provided code.\n    Time: 5 minutes\n    \"\"\"\n    # Broken code\n    def append_to_list_broken(item, target_list=[]):\n        \"\"\"This has a mutable default argument bug!\"\"\"\n        target_list.append(item)\n        return target_list\n    \n    # Demonstrate the bug\n    # list1 = append_to_list_broken(1)  # [1]\n    # list2 = append_to_list_broken(2)  # [1, 2] - UNEXPECTED!\n    \n    # Fixed solution\n    def append_to_list_fixed(item, target_list=None):\n        \"\"\"Fixed version using None as default\"\"\"\n        if target_list is None:\n            target_list = []\n        target_list.append(item)\n        return target_list\n    \n    # Test the fix\n    list1 = append_to_list_fixed(1)  # [1]\n    list2 = append_to_list_fixed(2)  # [2] - CORRECT!\n    list3 = append_to_list_fixed(3, list1)  # [1, 3] - CORRECT!\n    \n    print(f\"list1: {list1}, list2: {list2}, list3: {list3}\")\n    \n    # Explanation: Default mutable arguments are created once at function\n    # definition time, not at call time!\n    \n    return append_to_list_fixed\n\ndef exercise_3_5():\n    \"\"\"\n    INTERVIEW: Use map, filter, and reduce to process a list of transactions.\n    Calculate total revenue from valid transactions (amount > 0, status='completed').\n    Time: 10 minutes\n    \"\"\"\n    # Problem\n    transactions = [\n        {'id': 1, 'amount': 100, 'status': 'completed'},\n        {'id': 2, 'amount': -50, 'status': 'completed'},  # Invalid: negative\n        {'id': 3, 'amount': 200, 'status': 'pending'},    # Invalid: pending\n        {'id': 4, 'amount': 150, 'status': 'completed'},\n        {'id': 5, 'amount': 0, 'status': 'completed'},    # Invalid: zero\n        {'id': 6, 'amount': 300, 'status': 'completed'},\n    ]\n    \n    # Solution using functional approach\n    from functools import reduce\n    \n    # Step 1: Filter valid transactions\n    valid_transactions = list(filter(\n        lambda t: t['amount'] > 0 and t['status'] == 'completed',\n        transactions\n    ))\n    \n    # Step 2: Extract amounts using map\n    amounts = list(map(lambda t: t['amount'], valid_transactions))\n    \n    # Step 3: Sum using reduce\n    total_revenue = reduce(lambda x, y: x + y, amounts, 0)\n    \n    print(f\"Valid transactions: {len(valid_transactions)}\")\n    print(f\"Total revenue: ${total_revenue}\")  # $550\n    \n    # One-liner version (less readable but impressive)\n    total_oneliner = reduce(\n        lambda acc, t: acc + t['amount'],\n        filter(\n            lambda t: t['amount'] > 0 and t['status'] == 'completed',\n            transactions\n        ),\n        0\n    )\n    \n    # Pandas alternative (more practical)\n    df = pd.DataFrame(transactions)\n    total_pandas = df[\n        (df['amount'] > 0) & (df['status'] == 'completed')\n    ]['amount'].sum()\n    \n    return total_revenue\n\n# ============================================================================\n# MODULE 4: ESSENTIAL PANDAS OPERATIONS\n# ============================================================================\n\ndef exercise_4_1():\n    \"\"\"\n    EASY: Create a DataFrame and calculate mean salary by department.\n    Time: 5 minutes\n    \"\"\"\n    # Problem data\n    data = {\n        'name': ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve', 'Frank'],\n        'department': ['Sales', 'IT', 'Sales', 'IT', 'HR', 'IT'],\n        'salary': [50000, 75000, 55000, 80000, 45000, 70000]\n    }\n    \n    # Solution\n    df = pd.DataFrame(data)\n    \n    # Calculate mean by department\n    mean_salaries = df.groupby('department')['salary'].mean()\n    \n    # Alternative: Get as DataFrame with reset index\n    mean_salaries_df = df.groupby('department')['salary'].mean().reset_index()\n    mean_salaries_df.columns = ['department', 'avg_salary']\n    \n    print(\"Mean salaries by department:\")\n    print(mean_salaries)\n    \n    return mean_salaries\n\ndef exercise_4_2():\n    \"\"\"\n    MEDIUM: Merge two DataFrames and handle missing values in the result.\n    Time: 10 minutes\n    \"\"\"\n    # Problem data\n    employees = pd.DataFrame({\n        'emp_id': [1, 2, 3, 4, 5],\n        'name': ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'],\n        'dept_id': [10, 20, 10, 30, 20]\n    })\n    \n    departments = pd.DataFrame({\n        'dept_id': [10, 20, 40],  # Note: no dept 30, but has 40\n        'dept_name': ['Sales', 'IT', 'Marketing'],\n        'budget': [100000, 200000, 150000]\n    })\n    \n    # Solution\n    # Left join to keep all employees\n    merged = pd.merge(employees, departments, on='dept_id', how='left')\n    \n    # Handle missing values\n    merged['dept_name'] = merged['dept_name'].fillna('Unknown')\n    merged['budget'] = merged['budget'].fillna(0)\n    \n    # Alternative: Outer join to see all departments too\n    merged_outer = pd.merge(employees, departments, on='dept_id', how='outer')\n    merged_outer['name'] = merged_outer['name'].fillna('No Employee')\n    \n    print(\"Merged with missing value handling:\")\n    print(merged)\n    \n    return merged\n\ndef exercise_4_3():\n    \"\"\"\n    HARD: Group by multiple columns and calculate multiple aggregations.\n    Time: 12 minutes\n    \"\"\"\n    # Problem data - sales transactions\n    sales = pd.DataFrame({\n        'date': pd.date_range('2024-01-01', periods=100, freq='D'),\n        'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n        'product': np.random.choice(['A', 'B', 'C'], 100),\n        'quantity': np.random.randint(1, 20, 100),\n        'price': np.random.uniform(10, 100, 100).round(2)\n    })\n    sales['revenue'] = sales['quantity'] * sales['price']\n    sales['month'] = sales['date'].dt.month\n    \n    # Solution\n    # Group by region and product, multiple aggregations\n    summary = sales.groupby(['region', 'product']).agg({\n        'quantity': ['sum', 'mean'],\n        'revenue': ['sum', 'mean', 'max'],\n        'price': ['mean', 'std']\n    }).round(2)\n    \n    # Flatten column names\n    summary.columns = ['_'.join(col) for col in summary.columns]\n    summary = summary.reset_index()\n    \n    # Alternative: Named aggregations (cleaner)\n    summary_clean = sales.groupby(['region', 'product']).agg(\n        total_quantity=('quantity', 'sum'),\n        avg_quantity=('quantity', 'mean'),\n        total_revenue=('revenue', 'sum'),\n        avg_revenue=('revenue', 'mean'),\n        max_revenue=('revenue', 'max'),\n        avg_price=('price', 'mean')\n    ).round(2).reset_index()\n    \n    print(\"Summary statistics:\")\n    print(summary_clean.head(10))\n    \n    return summary_clean\n\ndef exercise_4_4():\n    \"\"\"\n    DEBUG: Fix the SettingWithCopyWarning in the provided code.\n    Time: 7 minutes\n    \"\"\"\n    # Broken code that causes warning\n    def process_data_broken(df):\n        # Filter data\n        high_value = df[df['value'] > 100]\n        \n        # This causes SettingWithCopyWarning!\n        high_value['category'] = 'Premium'  # BAD\n        \n        return high_value\n    \n    # Fixed solution\n    def process_data_fixed(df):\n        # Method 1: Use .copy() explicitly\n        high_value = df[df['value'] > 100].copy()\n        high_value['category'] = 'Premium'\n        \n        # Method 2: Use .loc for assignment\n        df.loc[df['value'] > 100, 'category'] = 'Premium'\n        \n        # Method 3: Assign with explicit copy\n        high_value = df.loc[df['value'] > 100, :].copy()\n        high_value['category'] = 'Premium'\n        \n        return high_value\n    \n    # Test\n    test_df = pd.DataFrame({\n        'value': [50, 150, 200, 75, 300],\n        'name': ['A', 'B', 'C', 'D', 'E']\n    })\n    \n    result = process_data_fixed(test_df.copy())\n    print(\"Fixed result without warning:\")\n    print(result)\n    \n    return result\n\ndef exercise_4_5():\n    \"\"\"\n    INTERVIEW: Find top 3 products by revenue in each region, \n    excluding nulls and products with less than 10 total sales.\n    Time: 15 minutes\n    \"\"\"\n    # Generate realistic sales data\n    np.random.seed(42)\n    n_records = 1000\n    \n    sales_data = pd.DataFrame({\n        'product_id': np.random.choice(['P001', 'P002', 'P003', 'P004', 'P005', \n                                       'P006', 'P007', 'P008', None], n_records),\n        'region': np.random.choice(['North', 'South', 'East', 'West', None], n_records),\n        'quantity': np.random.randint(1, 20, n_records),\n        'price': np.random.uniform(10, 100, n_records)\n    })\n    \n    # Add some products with low sales\n    sales_data.loc[sales_data['product_id'] == 'P008', 'quantity'] = 1\n    \n    # Calculate revenue\n    sales_data['revenue'] = sales_data['quantity'] * sales_data['price']\n    \n    # Solution\n    # Step 1: Remove nulls\n    clean_data = sales_data.dropna(subset=['product_id', 'region'])\n    \n    # Step 2: Calculate total sales per product\n    product_sales = clean_data.groupby('product_id')['quantity'].sum()\n    valid_products = product_sales[product_sales >= 10].index\n    \n    # Step 3: Filter for valid products only\n    valid_data = clean_data[clean_data['product_id'].isin(valid_products)]\n    \n    # Step 4: Calculate total revenue by region and product\n    revenue_summary = valid_data.groupby(['region', 'product_id'])['revenue'].sum().reset_index()\n    \n    # Step 5: Find top 3 per region\n    top_products = (revenue_summary\n                    .sort_values('revenue', ascending=False)\n                    .groupby('region')\n                    .head(3))\n    \n    # Alternative: Using rank\n    revenue_summary['rank'] = (revenue_summary\n                               .groupby('region')['revenue']\n                               .rank(method='dense', ascending=False))\n    top_products_alt = revenue_summary[revenue_summary['rank'] <= 3]\n    \n    print(\"Top 3 products by revenue per region:\")\n    print(top_products.sort_values(['region', 'revenue'], ascending=[True, False]))\n    \n    return top_products\n\n# ============================================================================\n# MODULE 5: FILE I/O & ERROR HANDLING\n# ============================================================================\n\ndef exercise_5_1():\n    \"\"\"\n    EASY: Write a function that safely reads a JSON file \n    and returns empty dict on error.\n    Time: 5 minutes\n    \"\"\"\n    # Solution\n    def read_json_safe(filepath):\n        \"\"\"\n        Safely read JSON file with error handling.\n        Returns empty dict if any error occurs.\n        \"\"\"\n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n            return data\n        except FileNotFoundError:\n            print(f\"File not found: {filepath}\")\n            return {}\n        except json.JSONDecodeError as e:\n            print(f\"Invalid JSON in {filepath}: {e}\")\n            return {}\n        except Exception as e:\n            print(f\"Unexpected error reading {filepath}: {e}\")\n            return {}\n    \n    # Test with various scenarios\n    # result1 = read_json_safe('exists.json')  # Normal read\n    # result2 = read_json_safe('missing.json')  # Returns {}\n    # result3 = read_json_safe('invalid.json')  # Returns {}\n    \n    print(\"Function created: read_json_safe()\")\n    return read_json_safe\n\ndef exercise_5_2():\n    \"\"\"\n    MEDIUM: Read a CSV file and handle potential encoding errors.\n    Time: 8 minutes\n    \"\"\"\n    # Solution\n    def read_csv_with_encoding(filepath):\n        \"\"\"\n        Try multiple encodings to read a CSV file.\n        \"\"\"\n        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'utf-16']\n        \n        for encoding in encodings:\n            try:\n                df = pd.read_csv(filepath, encoding=encoding)\n                print(f\"Successfully read with {encoding} encoding\")\n                return df\n            except UnicodeDecodeError:\n                continue\n            except FileNotFoundError:\n                print(f\"File not found: {filepath}\")\n                return pd.DataFrame()\n        \n        # If all encodings fail\n        print(f\"Could not read {filepath} with any standard encoding\")\n        \n        # Last resort: ignore errors (may lose some characters)\n        try:\n            df = pd.read_csv(filepath, encoding='utf-8', errors='ignore')\n            print(\"Read with errors ignored (some characters may be lost)\")\n            return df\n        except Exception as e:\n            print(f\"Failed to read file: {e}\")\n            return pd.DataFrame()\n    \n    print(\"Function created: read_csv_with_encoding()\")\n    return read_csv_with_encoding\n\ndef exercise_5_3():\n    \"\"\"\n    HARD: Implement retry logic with exponential backoff for API calls.\n    Time: 12 minutes\n    \"\"\"\n    # Solution\n    import time\n    import random\n    \n    def api_call_with_retry(\n        api_func, \n        max_retries=3, \n        initial_delay=1,\n        max_delay=32,\n        exponential_base=2,\n        jitter=True\n    ):\n        \"\"\"\n        Call an API function with retry logic and exponential backoff.\n        \n        Args:\n            api_func: Function to call\n            max_retries: Maximum number of retry attempts\n            initial_delay: Initial delay in seconds\n            max_delay: Maximum delay in seconds\n            exponential_base: Base for exponential backoff\n            jitter: Add random jitter to prevent thundering herd\n        \"\"\"\n        delay = initial_delay\n        \n        for attempt in range(max_retries + 1):\n            try:\n                result = api_func()\n                if attempt > 0:\n                    print(f\"Success after {attempt} retries\")\n                return result\n                \n            except Exception as e:\n                if attempt == max_retries:\n                    print(f\"Failed after {max_retries} retries: {e}\")\n                    raise\n                \n                # Calculate next delay\n                if jitter:\n                    actual_delay = delay * (0.5 + random.random())\n                else:\n                    actual_delay = delay\n                \n                print(f\"Attempt {attempt + 1} failed: {e}\")\n                print(f\"Retrying in {actual_delay:.2f} seconds...\")\n                \n                time.sleep(actual_delay)\n                \n                # Exponential backoff\n                delay = min(delay * exponential_base, max_delay)\n        \n        raise Exception(\"Retry logic error - should not reach here\")\n    \n    # Example API function that fails sometimes\n    def flaky_api():\n        if random.random() < 0.7:  # 70% failure rate\n            raise ConnectionError(\"API temporarily unavailable\")\n        return {\"status\": \"success\", \"data\": [1, 2, 3]}\n    \n    # Test the retry logic\n    # result = api_call_with_retry(flaky_api)\n    \n    print(\"Function created: api_call_with_retry()\")\n    return api_call_with_retry\n\ndef exercise_5_4():\n    \"\"\"\n    DEBUG: Fix the file handling code that doesn't properly close files.\n    Time: 5 minutes\n    \"\"\"\n    # Broken code\n    def process_file_broken(filepath):\n        \"\"\"This doesn't properly close the file on error!\"\"\"\n        file = open(filepath, 'r')\n        data = file.read()\n        \n        # If error occurs here, file never closes!\n        processed = data.upper()\n        \n        file.close()\n        return processed\n    \n    # Fixed solution 1: try/finally\n    def process_file_fixed_v1(filepath):\n        \"\"\"Using try/finally to ensure file closure\"\"\"\n        file = None\n        try:\n            file = open(filepath, 'r')\n            data = file.read()\n            processed = data.upper()\n            return processed\n        finally:\n            if file:\n                file.close()\n    \n    # Fixed solution 2: context manager (BEST)\n    def process_file_fixed_v2(filepath):\n        \"\"\"Using context manager (with statement) - Pythonic way\"\"\"\n        with open(filepath, 'r') as file:\n            data = file.read()\n            processed = data.upper()\n        # File automatically closed here, even if error occurs\n        return processed\n    \n    print(\"Fixed versions created using context manager\")\n    return process_file_fixed_v2\n\ndef exercise_5_5():\n    \"\"\"\n    INTERVIEW: Process large CSV in chunks and aggregate results.\n    Calculate average salary by department from a 10GB file.\n    Time: 15 minutes\n    \"\"\"\n    # Solution\n    def process_large_csv(filepath, chunk_size=10000):\n        \"\"\"\n        Process large CSV file in chunks to calculate aggregations.\n        Memory-efficient approach for files that don't fit in memory.\n        \"\"\"\n        # Initialize aggregators\n        department_sums = {}\n        department_counts = {}\n        \n        # Process in chunks\n        chunk_num = 0\n        \n        try:\n            for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n                chunk_num += 1\n                print(f\"Processing chunk {chunk_num}...\")\n                \n                # Aggregate within chunk\n                chunk_agg = chunk.groupby('department').agg({\n                    'salary': ['sum', 'count']\n                })\n                \n                # Merge with overall aggregation\n                for dept in chunk_agg.index:\n                    salary_sum = chunk_agg.loc[dept, ('salary', 'sum')]\n                    salary_count = chunk_agg.loc[dept, ('salary', 'count')]\n                    \n                    if dept in department_sums:\n                        department_sums[dept] += salary_sum\n                        department_counts[dept] += salary_count\n                    else:\n                        department_sums[dept] = salary_sum\n                        department_counts[dept] = salary_count\n        \n        except FileNotFoundError:\n            print(f\"File not found: {filepath}\")\n            return pd.DataFrame()\n        \n        # Calculate final averages\n        result = pd.DataFrame({\n            'department': list(department_sums.keys()),\n            'total_salary': list(department_sums.values()),\n            'employee_count': list(department_counts.values())\n        })\n        \n        result['average_salary'] = result['total_salary'] / result['employee_count']\n        \n        print(f\"Processed {chunk_num} chunks\")\n        print(f\"Found {len(result)} departments\")\n        \n        return result[['department', 'average_salary', 'employee_count']]\n    \n    # Alternative: Using dask for very large files\n    def process_with_dask(filepath):\n        \"\"\"Alternative using dask for parallel processing\"\"\"\n        # import dask.dataframe as dd\n        # ddf = dd.read_csv(filepath)\n        # result = ddf.groupby('department')['salary'].mean().compute()\n        pass\n    \n    print(\"Function created: process_large_csv()\")\n    return process_large_csv\n\n# ============================================================================\n# MODULE 6: COMMON GOTCHAS & BEST PRACTICES\n# ============================================================================\n\ndef exercise_6_1():\n    \"\"\"\n    EASY: Implement bubble sort without using .sort() or sorted().\n    Time: 8 minutes\n    \"\"\"\n    # Solution\n    def bubble_sort(arr):\n        \"\"\"\n        Implement bubble sort manually.\n        Modifies list in-place and returns it.\n        \"\"\"\n        n = len(arr)\n        \n        # Make a copy to avoid modifying original\n        result = arr.copy()\n        \n        # Bubble sort algorithm\n        for i in range(n):\n            # Flag to optimize by detecting if already sorted\n            swapped = False\n            \n            # Last i elements are already in place\n            for j in range(0, n - i - 1):\n                if result[j] > result[j + 1]:\n                    # Swap elements\n                    result[j], result[j + 1] = result[j + 1], result[j]\n                    swapped = True\n            \n            # If no swaps occurred, array is sorted\n            if not swapped:\n                break\n        \n        return result\n    \n    # Test\n    test_data = [64, 34, 25, 12, 22, 11, 90]\n    sorted_data = bubble_sort(test_data)\n    \n    print(f\"Original: {test_data}\")\n    print(f\"Sorted: {sorted_data}\")\n    \n    # Alternative: Selection sort (also O(n\u00b2))\n    def selection_sort(arr):\n        result = arr.copy()\n        n = len(result)\n        \n        for i in range(n):\n            min_idx = i\n            for j in range(i + 1, n):\n                if result[j] < result[min_idx]:\n                    min_idx = j\n            result[i], result[min_idx] = result[min_idx], result[i]\n        \n        return result\n    \n    return bubble_sort\n\ndef exercise_6_2():\n    \"\"\"\n    MEDIUM: Group a list of dictionaries by a key without using groupby.\n    Time: 10 minutes\n    \"\"\"\n    # Problem data\n    records = [\n        {'name': 'Alice', 'department': 'Sales', 'salary': 50000},\n        {'name': 'Bob', 'department': 'IT', 'salary': 75000},\n        {'name': 'Charlie', 'department': 'Sales', 'salary': 55000},\n        {'name': 'Dave', 'department': 'IT', 'salary': 80000},\n        {'name': 'Eve', 'department': 'HR', 'salary': 45000},\n    ]\n    \n    # Solution\n    def manual_groupby(data, key_field):\n        \"\"\"\n        Manually group records by a specified field.\n        Returns dict with keys as group values and values as lists of records.\n        \"\"\"\n        groups = {}\n        \n        for record in data:\n            key = record.get(key_field)\n            \n            if key not in groups:\n                groups[key] = []\n            \n            groups[key].append(record)\n        \n        return groups\n    \n    # Group by department\n    grouped = manual_groupby(records, 'department')\n    \n    # Calculate aggregations manually\n    dept_stats = {}\n    for dept, employees in grouped.items():\n        total_salary = sum(emp['salary'] for emp in employees)\n        avg_salary = total_salary / len(employees)\n        \n        dept_stats[dept] = {\n            'count': len(employees),\n            'total_salary': total_salary,\n            'avg_salary': avg_salary,\n            'employees': [emp['name'] for emp in employees]\n        }\n    \n    print(\"Grouped by department:\")\n    for dept, stats in dept_stats.items():\n        print(f\"{dept}: {stats}\")\n    \n    return grouped\n\ndef exercise_6_3():\n    \"\"\"\n    HARD: Flatten an arbitrarily nested list structure.\n    Time: 12 minutes\n    \"\"\"\n    # Solution\n    def flatten_recursive(nested):\n        \"\"\"\n        Flatten nested list recursively.\n        Handles arbitrary nesting depth.\n        \"\"\"\n        result = []\n        \n        for item in nested:\n            if isinstance(item, list):\n                # Recursively flatten sublists\n                result.extend(flatten_recursive(item))\n            else:\n                result.append(item)\n        \n        return result\n    \n    def flatten_iterative(nested):\n        \"\"\"\n        Flatten nested list iteratively using a stack.\n        Alternative approach without recursion.\n        \"\"\"\n        result = []\n        stack = [nested]\n        \n        while stack:\n            current = stack.pop()\n            \n            if isinstance(current, list):\n                # Add list elements to stack in reverse order\n                # so they're processed in original order\n                stack.extend(reversed(current))\n            else:\n                result.append(current)\n        \n        # Reverse since we built it backwards\n        return result[::-1]\n    \n    def flatten_generator(nested):\n        \"\"\"\n        Memory-efficient generator version.\n        \"\"\"\n        for item in nested:\n            if isinstance(item, list):\n                yield from flatten_generator(item)\n            else:\n                yield item\n    \n    # Test with complex nesting\n    complex_nested = [1, [2, 3], [4, [5, [6, 7]], 8], [[[[9]]]], 10]\n    \n    flat1 = flatten_recursive(complex_nested)\n    flat2 = flatten_iterative(complex_nested)\n    flat3 = list(flatten_generator(complex_nested))\n    \n    print(f\"Original: {complex_nested}\")\n    print(f\"Flattened: {flat1}\")\n    assert flat1 == flat2 == flat3 == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    \n    return flatten_recursive\n\ndef exercise_6_4():\n    \"\"\"\n    DEBUG: Fix the code that modifies a list while iterating.\n    Time: 5 minutes\n    \"\"\"\n    # Broken code\n    def remove_negatives_broken(numbers):\n        \"\"\"This will skip elements!\"\"\"\n        for i, num in enumerate(numbers):\n            if num < 0:\n                del numbers[i]  # BAD: Modifying list during iteration\n        return numbers\n    \n    # Test the broken version to see the issue\n    test_broken = [-1, -2, 3, -4, 5, -6]\n    # result = remove_negatives_broken(test_broken.copy())\n    # Result would be [-2, 3, 5, -6] - WRONG! Skipped -2 and -6\n    \n    # Fixed solutions\n    def remove_negatives_v1(numbers):\n        \"\"\"Use list comprehension (best)\"\"\"\n        return [num for num in numbers if num >= 0]\n    \n    def remove_negatives_v2(numbers):\n        \"\"\"Iterate over copy\"\"\"\n        result = numbers.copy()\n        for num in numbers:  # Iterate over original\n            if num < 0:\n                result.remove(num)  # Modify copy\n        return result\n    \n    def remove_negatives_v3(numbers):\n        \"\"\"Iterate backwards (index-safe)\"\"\"\n        for i in range(len(numbers) - 1, -1, -1):\n            if numbers[i] < 0:\n                del numbers[i]\n        return numbers\n    \n    def remove_negatives_v4(numbers):\n        \"\"\"Use filter\"\"\"\n        return list(filter(lambda x: x >= 0, numbers))\n    \n    # Test all versions\n    test_data = [-1, -2, 3, -4, 5, -6]\n    \n    result1 = remove_negatives_v1(test_data.copy())\n    result2 = remove_negatives_v2(test_data.copy())\n    result3 = remove_negatives_v3(test_data.copy())\n    result4 = remove_negatives_v4(test_data.copy())\n    \n    print(f\"Original: {test_data}\")\n    print(f\"Fixed result: {result1}\")\n    assert result1 == result2 == result3 == result4 == [3, 5]\n    \n    return remove_negatives_v1\n\ndef exercise_6_5():\n    \"\"\"\n    INTERVIEW: Optimize slow pandas code to use vectorized operations.\n    Time: 15 minutes\n    \"\"\"\n    # Generate test data\n    np.random.seed(42)\n    n = 100000\n    df = pd.DataFrame({\n        'category': np.random.choice(['A', 'B', 'C'], n),\n        'value': np.random.randn(n) * 100,\n        'quantity': np.random.randint(1, 100, n)\n    })\n    \n    # SLOW VERSION - Iterating over rows\n    def calculate_metrics_slow(df):\n        \"\"\"Slow version using iteration\"\"\"\n        results = []\n        \n        for index, row in df.iterrows():\n            if row['category'] == 'A':\n                multiplier = 1.2\n            elif row['category'] == 'B':\n                multiplier = 1.5\n            else:\n                multiplier = 1.0\n            \n            adjusted_value = row['value'] * multiplier\n            total = adjusted_value * row['quantity']\n            \n            results.append({\n                'index': index,\n                'adjusted_value': adjusted_value,\n                'total': total\n            })\n        \n        return pd.DataFrame(results)\n    \n    # FAST VERSION - Vectorized operations\n    def calculate_metrics_fast(df):\n        \"\"\"Fast version using vectorized operations\"\"\"\n        # Create multiplier column using numpy.where\n        df['multiplier'] = np.where(\n            df['category'] == 'A', 1.2,\n            np.where(df['category'] == 'B', 1.5, 1.0)\n        )\n        \n        # Vectorized calculations\n        df['adjusted_value'] = df['value'] * df['multiplier']\n        df['total'] = df['adjusted_value'] * df['quantity']\n        \n        return df[['adjusted_value', 'total']]\n    \n    # FASTEST VERSION - Using pandas methods\n    def calculate_metrics_fastest(df):\n        \"\"\"Fastest using map for category lookup\"\"\"\n        multiplier_map = {'A': 1.2, 'B': 1.5, 'C': 1.0}\n        \n        df['adjusted_value'] = df['value'] * df['category'].map(multiplier_map)\n        df['total'] = df['adjusted_value'] * df['quantity']\n        \n        return df[['adjusted_value', 'total']]\n    \n    # Performance comparison (don't run on large data in exercises)\n    import time\n    \n    # Test on small sample\n    sample = df.head(1000).copy()\n    \n    # Slow version\n    # start = time.time()\n    # result_slow = calculate_metrics_slow(sample.copy())\n    # slow_time = time.time() - start\n    \n    # Fast version\n    start = time.time()\n    result_fast = calculate_metrics_fast(sample.copy())\n    fast_time = time.time() - start\n    \n    # Fastest version\n    start = time.time()\n    result_fastest = calculate_metrics_fastest(sample.copy())\n    fastest_time = time.time() - start\n    \n    print(f\"Vectorized version time: {fast_time:.4f}s\")\n    print(f\"Map version time: {fastest_time:.4f}s\")\n    print(\"Vectorized operations are 100-1000x faster than iteration!\")\n    \n    # Key optimization principles demonstrated:\n    # 1. Avoid iterrows() - extremely slow\n    # 2. Use numpy.where for conditional logic\n    # 3. Use map() for lookups\n    # 4. Leverage pandas built-in vectorized operations\n    # 5. Avoid applying Python functions row-by-row\n    \n    return calculate_metrics_fastest\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\ndef run_all_exercises():\n    \"\"\"\n    Run all exercises with timing and scoring.\n    \"\"\"\n    exercises = [\n        # Module 1\n        exercise_1_1, exercise_1_2, exercise_1_3, exercise_1_4, exercise_1_5,\n        # Module 2\n        exercise_2_1, exercise_2_2, exercise_2_3, exercise_2_4, exercise_2_5,\n        # Module 3\n        exercise_3_1, exercise_3_2, exercise_3_3, exercise_3_4, exercise_3_5,\n        # Module 4\n        exercise_4_1, exercise_4_2, exercise_4_3, exercise_4_4, exercise_4_5,\n        # Module 5\n        exercise_5_1, exercise_5_2, exercise_5_3, exercise_5_4, exercise_5_5,\n        # Module 6\n        exercise_6_1, exercise_6_2, exercise_6_3, exercise_6_4, exercise_6_5,\n    ]\n    \n    print(\"=\" * 80)\n    print(\"PYTHON ANALYTICS ENGINEERING INTERVIEW PREP - EXERCISES\")\n    print(\"=\" * 80)\n    print(\"\\nTotal exercises: 60 (10 per module)\")\n    print(\"\\nDifficulty distribution per module:\")\n    print(\"- Easy: 2 exercises\")\n    print(\"- Medium: 2 exercises\") \n    print(\"- Hard: 2 exercises\")\n    print(\"- Debug: 2 exercises\")\n    print(\"- Interview: 2 exercises\")\n    print(\"\\n\" + \"=\" * 80)\n    \n    for i, exercise in enumerate(exercises, 1):\n        module = (i - 1) // 10 + 1\n        exercise_num = (i - 1) % 10 + 1\n        \n        if exercise_num == 1:\n            print(f\"\\n{'=' * 40}\")\n            print(f\"MODULE {module}\")\n            print(f\"{'=' * 40}\")\n        \n        print(f\"\\nExercise {module}.{exercise_num}: {exercise.__name__}\")\n        print(\"-\" * 40)\n        \n        try:\n            result = exercise()\n            print(\"\u2713 Exercise completed\")\n        except Exception as e:\n            print(f\"\u2717 Error: {e}\")\n        \n        print(\"-\" * 40)\n\nif __name__ == \"__main__\":\n    # Run specific exercise for testing\n    # exercise_1_1()\n    \n    # Or run all exercises\n    # run_all_exercises()\n    \n    print(\"\\nExercises file loaded successfully!\")\n    print(\"Run specific exercises: exercise_1_1(), exercise_2_3(), etc.\")\n    print(\"Run all exercises: run_all_exercises()\")\n",
  "src/patterns_and_gotchas.py": "#!/usr/bin/env python3\n\"\"\"\nPatterns and Gotchas - Complete Implementation Reference\nAnalytics Engineering Interview Patterns\n\nThis file contains clean, ready-to-use implementations of:\n1. Core pandas patterns (20 most common)\n2. Gotcha implementations (manual versions without built-ins)\n3. Optimized versions showing performance differences\n\nCopy and adapt these patterns during interviews.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Any, Tuple\nfrom collections import defaultdict, Counter\nimport time\n\n# ============================================================================\n# PART 1: CORE PANDAS PATTERNS\n# ============================================================================\n\nclass CorePatterns:\n    \"\"\"The 20 patterns that solve 80% of interview problems.\"\"\"\n    \n    @staticmethod\n    def top_n_by_group(df: pd.DataFrame, group_col: str, value_col: str, n: int = 5) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Find top N items by value within each group.\n        Common variations: top products by region, best performers by department\n        \"\"\"\n        # Method 1: Sort then groupby head (most intuitive)\n        result = (df\n                 .sort_values(value_col, ascending=False)\n                 .groupby(group_col)\n                 .head(n))\n        \n        # Method 2: Using nlargest (more efficient for large data)\n        result_alt = (df\n                     .groupby(group_col)\n                     .apply(lambda x: x.nlargest(n, value_col))\n                     .reset_index(drop=True))\n        \n        return result\n    \n    @staticmethod\n    def groupby_multiple_agg(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Group by columns with multiple aggregations.\n        Common: Revenue reports, summary statistics\n        \"\"\"\n        # Method 1: Dictionary of aggregations\n        result = df.groupby('category').agg({\n            'value': ['sum', 'mean', 'count'],\n            'quantity': ['sum', 'mean'],\n            'price': ['min', 'max', 'mean']\n        }).round(2)\n        \n        # Method 2: Named aggregations (cleaner columns)\n        result_clean = df.groupby('category').agg(\n            total_value=('value', 'sum'),\n            avg_value=('value', 'mean'),\n            count=('value', 'count'),\n            total_quantity=('quantity', 'sum')\n        ).reset_index()\n        \n        return result_clean\n    \n    @staticmethod\n    def merge_with_indicator(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Merge dataframes and track source of records.\n        Common: Finding unmatched records, data validation\n        \"\"\"\n        merged = pd.merge(\n            df1, df2, \n            on='key', \n            how='outer', \n            indicator=True\n        )\n        \n        # Analyze merge results\n        merge_summary = merged['_merge'].value_counts()\n        \n        # Get unmatched records\n        left_only = merged[merged['_merge'] == 'left_only']\n        right_only = merged[merged['_merge'] == 'right_only']\n        \n        return merged\n    \n    @staticmethod\n    def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Comprehensive missing value handling.\n        Common: Data cleaning step in every interview\n        \"\"\"\n        df = df.copy()\n        \n        # Strategy 1: Different fill for each column type\n        for col in df.columns:\n            if df[col].dtype == 'object':\n                # Categorical: fill with 'Unknown' or mode\n                df[col].fillna('Unknown', inplace=True)\n            elif df[col].dtype in ['int64', 'float64']:\n                # Numeric: fill with mean, median, or 0\n                df[col].fillna(df[col].median(), inplace=True)\n        \n        # Strategy 2: Forward/backward fill for time series\n        df_time = df.fillna(method='ffill').fillna(method='bfill')\n        \n        # Strategy 3: Drop if too many missing\n        threshold = len(df) * 0.5  # Keep columns with >50% data\n        df_clean = df.dropna(thresh=threshold, axis=1)\n        \n        return df\n    \n    @staticmethod\n    def remove_duplicates_advanced(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Remove duplicates with various strategies.\n        Common: Data quality checks\n        \"\"\"\n        # Keep first occurrence\n        df1 = df.drop_duplicates(keep='first')\n        \n        # Keep last occurrence  \n        df2 = df.drop_duplicates(keep='last')\n        \n        # Drop all duplicates (keep none)\n        df3 = df.drop_duplicates(keep=False)\n        \n        # Based on subset of columns\n        df4 = df.drop_duplicates(subset=['col1', 'col2'], keep='first')\n        \n        # Find duplicates for investigation\n        duplicates = df[df.duplicated(keep=False)]\n        \n        return df1\n    \n    @staticmethod\n    def rolling_calculations(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Rolling/moving window calculations.\n        Common: Moving averages, trend analysis\n        \"\"\"\n        df = df.copy()\n        \n        # Simple rolling mean\n        df['rolling_mean_7'] = df['value'].rolling(window=7).mean()\n        \n        # Rolling with min periods (handles start of series)\n        df['rolling_mean_safe'] = df['value'].rolling(window=7, min_periods=1).mean()\n        \n        # Multiple rolling calculations\n        df['rolling_std'] = df['value'].rolling(window=7).std()\n        df['rolling_max'] = df['value'].rolling(window=7).max()\n        df['rolling_min'] = df['value'].rolling(window=7).min()\n        \n        # Expanding window (cumulative)\n        df['expanding_mean'] = df['value'].expanding().mean()\n        \n        return df\n    \n    @staticmethod\n    def pivot_table_pattern(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Reshape data with pivot tables.\n        Common: Creating reports, cross-tabulations\n        \"\"\"\n        # Basic pivot\n        pivot1 = df.pivot_table(\n            values='sales',\n            index='region',\n            columns='product',\n            aggfunc='sum',\n            fill_value=0\n        )\n        \n        # Multiple aggregations\n        pivot2 = df.pivot_table(\n            values='sales',\n            index='region',\n            columns='product',\n            aggfunc=['sum', 'mean', 'count'],\n            fill_value=0\n        )\n        \n        # Multiple values\n        pivot3 = df.pivot_table(\n            values=['sales', 'quantity'],\n            index='region',\n            columns='product',\n            aggfunc='sum',\n            fill_value=0\n        )\n        \n        return pivot1\n    \n    @staticmethod\n    def rank_within_groups(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Rank items within groups.\n        Common: Performance rankings, percentiles\n        \"\"\"\n        df = df.copy()\n        \n        # Dense rank (1,2,2,3)\n        df['rank_dense'] = df.groupby('group')['value'].rank(method='dense', ascending=False)\n        \n        # Min rank (1,2,2,4)\n        df['rank_min'] = df.groupby('group')['value'].rank(method='min', ascending=False)\n        \n        # Percentile rank\n        df['percentile'] = df.groupby('group')['value'].rank(pct=True)\n        \n        # Top 3 flag\n        df['is_top3'] = df['rank_dense'] <= 3\n        \n        return df\n    \n    @staticmethod\n    def cumulative_calculations(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Cumulative aggregations by group.\n        Common: Running totals, YTD calculations\n        \"\"\"\n        df = df.copy()\n        \n        # Cumulative sum by group\n        df['cumsum'] = df.groupby('group')['value'].cumsum()\n        \n        # Cumulative max\n        df['cummax'] = df.groupby('group')['value'].cummax()\n        \n        # Cumulative count\n        df['cumcount'] = df.groupby('group').cumcount() + 1\n        \n        # Percentage of total\n        df['pct_of_group_total'] = (\n            df.groupby('group')['value']\n            .transform(lambda x: x / x.sum() * 100)\n        )\n        \n        return df\n    \n    @staticmethod\n    def date_operations(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Common date/time operations.\n        Common: Time-based filtering and grouping\n        \"\"\"\n        df = df.copy()\n        \n        # Parse dates\n        df['date'] = pd.to_datetime(df['date_string'])\n        \n        # Extract components\n        df['year'] = df['date'].dt.year\n        df['month'] = df['date'].dt.month\n        df['quarter'] = df['date'].dt.quarter\n        df['weekday'] = df['date'].dt.day_name()\n        df['is_weekend'] = df['date'].dt.dayofweek.isin([5, 6])\n        \n        # Date arithmetic\n        df['days_since'] = (pd.Timestamp.now() - df['date']).dt.days\n        df['month_start'] = df['date'].dt.to_period('M').dt.to_timestamp()\n        \n        # Resampling for time series\n        daily_avg = df.set_index('date').resample('D')['value'].mean()\n        \n        return df\n    \n    @staticmethod\n    def filter_patterns(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Complex filtering patterns.\n        Common: Data subsetting with multiple conditions\n        \"\"\"\n        # Multiple conditions with &, |\n        filtered1 = df[(df['value'] > 100) & (df['category'] == 'A')]\n        \n        # Using isin for multiple values\n        filtered2 = df[df['category'].isin(['A', 'B', 'C'])]\n        \n        # Using between for ranges\n        filtered3 = df[df['value'].between(10, 100)]\n        \n        # String pattern matching\n        filtered4 = df[df['name'].str.contains('pattern', case=False, na=False)]\n        \n        # Query method (cleaner for complex conditions)\n        filtered5 = df.query('value > 100 and category in [\"A\", \"B\"]')\n        \n        # Filter by quantiles\n        threshold = df['value'].quantile(0.9)\n        filtered6 = df[df['value'] > threshold]\n        \n        return filtered1\n    \n    @staticmethod\n    def apply_patterns(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Pattern: Using apply for custom transformations.\n        Common: Complex row-wise operations\n        \"\"\"\n        df = df.copy()\n        \n        # Apply to Series (column)\n        df['doubled'] = df['value'].apply(lambda x: x * 2)\n        \n        # Apply to DataFrame (row-wise)\n        df['row_sum'] = df.apply(lambda row: row['val1'] + row['val2'], axis=1)\n        \n        # Apply with conditions\n        df['category'] = df['value'].apply(\n            lambda x: 'High' if x > 100 else 'Medium' if x > 50 else 'Low'\n        )\n        \n        # Vectorized alternative (faster)\n        conditions = [\n            df['value'] > 100,\n            df['value'] > 50\n        ]\n        choices = ['High', 'Medium']\n        df['category_fast'] = np.select(conditions, choices, default='Low')\n        \n        return df\n\n# ============================================================================\n# PART 2: GOTCHA IMPLEMENTATIONS (Manual Versions)\n# ============================================================================\n\nclass GotchaImplementations:\n    \"\"\"Manual implementations of common operations - frequent interview questions.\"\"\"\n    \n    @staticmethod\n    def bubble_sort(arr: List[float]) -> List[float]:\n        \"\"\"\n        Implement sort without using .sort() or sorted().\n        Time: O(n\u00b2), Space: O(1)\n        \"\"\"\n        result = arr.copy()\n        n = len(result)\n        \n        for i in range(n):\n            swapped = False\n            for j in range(0, n - i - 1):\n                if result[j] > result[j + 1]:\n                    result[j], result[j + 1] = result[j + 1], result[j]\n                    swapped = True\n            \n            if not swapped:\n                break\n        \n        return result\n    \n    @staticmethod\n    def manual_groupby(data: List[Dict], key: str) -> Dict[Any, List]:\n        \"\"\"\n        Implement groupby without using pandas.\n        Returns dict with grouped data.\n        \"\"\"\n        groups = defaultdict(list)\n        \n        for record in data:\n            group_key = record.get(key)\n            groups[group_key].append(record)\n        \n        return dict(groups)\n    \n    @staticmethod\n    def manual_groupby_agg(data: List[Dict], group_key: str, value_key: str, agg_func: str) -> Dict:\n        \"\"\"\n        GroupBy with aggregation without pandas.\n        Supports 'sum', 'mean', 'count', 'min', 'max'\n        \"\"\"\n        groups = defaultdict(list)\n        \n        # Group data\n        for record in data:\n            key = record.get(group_key)\n            value = record.get(value_key)\n            if value is not None:\n                groups[key].append(value)\n        \n        # Apply aggregation\n        result = {}\n        for key, values in groups.items():\n            if agg_func == 'sum':\n                result[key] = sum(values)\n            elif agg_func == 'mean':\n                result[key] = sum(values) / len(values) if values else 0\n            elif agg_func == 'count':\n                result[key] = len(values)\n            elif agg_func == 'min':\n                result[key] = min(values) if values else None\n            elif agg_func == 'max':\n                result[key] = max(values) if values else None\n        \n        return result\n    \n    @staticmethod\n    def remove_duplicates_manual(items: List) -> List:\n        \"\"\"\n        Remove duplicates without using set() or drop_duplicates().\n        Preserves original order.\n        \"\"\"\n        seen = []\n        result = []\n        \n        for item in items:\n            # For unhashable types (like dicts), use different comparison\n            if item not in seen:\n                seen.append(item)\n                result.append(item)\n        \n        return result\n    \n    @staticmethod\n    def find_max_manual(numbers: List[float]) -> float:\n        \"\"\"\n        Find maximum without using max().\n        \"\"\"\n        if not numbers:\n            return None\n        \n        max_val = numbers[0]\n        for num in numbers[1:]:\n            if num > max_val:\n                max_val = num\n        \n        return max_val\n    \n    @staticmethod\n    def count_occurrences_manual(items: List) -> Dict:\n        \"\"\"\n        Count occurrences without using Counter or value_counts().\n        \"\"\"\n        counts = {}\n        \n        for item in items:\n            counts[item] = counts.get(item, 0) + 1\n        \n        return counts\n    \n    @staticmethod\n    def reverse_list_manual(lst: List) -> List:\n        \"\"\"\n        Reverse a list without using [::-1] or reverse().\n        \"\"\"\n        result = []\n        for i in range(len(lst) - 1, -1, -1):\n            result.append(lst[i])\n        \n        return result\n    \n    @staticmethod\n    def flatten_nested_manual(nested: List) -> List:\n        \"\"\"\n        Flatten arbitrarily nested list without using libraries.\n        \"\"\"\n        result = []\n        \n        def flatten_recursive(lst):\n            for item in lst:\n                if isinstance(item, list):\n                    flatten_recursive(item)\n                else:\n                    result.append(item)\n        \n        flatten_recursive(nested)\n        return result\n    \n    @staticmethod\n    def join_strings_manual(strings: List[str], delimiter: str = ',') -> str:\n        \"\"\"\n        Join strings without using .join().\n        \"\"\"\n        if not strings:\n            return ''\n        \n        result = strings[0]\n        for s in strings[1:]:\n            result += delimiter + s\n        \n        return result\n    \n    @staticmethod\n    def filter_list_manual(items: List, condition_func) -> List:\n        \"\"\"\n        Filter list without using filter() or list comprehension.\n        \"\"\"\n        result = []\n        \n        for item in items:\n            if condition_func(item):\n                result.append(item)\n        \n        return result\n    \n    @staticmethod\n    def zip_manual(list1: List, list2: List) -> List[Tuple]:\n        \"\"\"\n        Implement zip without using zip().\n        \"\"\"\n        result = []\n        min_len = min(len(list1), len(list2))\n        \n        for i in range(min_len):\n            result.append((list1[i], list2[i]))\n        \n        return result\n    \n    @staticmethod\n    def all_manual(items: List[bool]) -> bool:\n        \"\"\"\n        Check if all elements are True without using all().\n        \"\"\"\n        for item in items:\n            if not item:\n                return False\n        return True\n    \n    @staticmethod\n    def any_manual(items: List[bool]) -> bool:\n        \"\"\"\n        Check if any element is True without using any().\n        \"\"\"\n        for item in items:\n            if item:\n                return True\n        return False\n    \n    @staticmethod\n    def find_index_manual(lst: List, target) -> int:\n        \"\"\"\n        Find index of element without using .index().\n        Returns -1 if not found.\n        \"\"\"\n        for i, val in enumerate(lst):\n            if val == target:\n                return i\n        return -1\n    \n    @staticmethod\n    def sum_manual(numbers: List[float]) -> float:\n        \"\"\"\n        Sum values without using sum().\n        \"\"\"\n        total = 0\n        for num in numbers:\n            total += num\n        return total\n\n# ============================================================================\n# PART 3: OPTIMIZED VERSIONS - Performance Comparisons\n# ============================================================================\n\nclass OptimizedPatterns:\n    \"\"\"Shows vectorized vs loop versions for performance discussions.\"\"\"\n    \n    @staticmethod\n    def calculate_with_loops(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        SLOW: Using iterrows (never do this in production).\n        \"\"\"\n        df = df.copy()\n        results = []\n        \n        for index, row in df.iterrows():\n            if row['category'] == 'A':\n                value = row['amount'] * 1.2\n            elif row['category'] == 'B':\n                value = row['amount'] * 1.5\n            else:\n                value = row['amount']\n            results.append(value)\n        \n        df['adjusted'] = results\n        return df\n    \n    @staticmethod\n    def calculate_vectorized(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        FAST: Using numpy.where for vectorized operations.\n        100-1000x faster than loops.\n        \"\"\"\n        df = df.copy()\n        \n        df['adjusted'] = np.where(\n            df['category'] == 'A', df['amount'] * 1.2,\n            np.where(df['category'] == 'B', df['amount'] * 1.5, df['amount'])\n        )\n        \n        return df\n    \n    @staticmethod\n    def calculate_with_map(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        FAST: Using map for category lookups.\n        Good for many categories.\n        \"\"\"\n        df = df.copy()\n        \n        multipliers = {'A': 1.2, 'B': 1.5, 'C': 1.0}\n        df['adjusted'] = df['amount'] * df['category'].map(multipliers).fillna(1.0)\n        \n        return df\n    \n    @staticmethod\n    def filter_comparison():\n        \"\"\"\n        Compare different filtering methods.\n        \"\"\"\n        # Create test data\n        n = 100000\n        df = pd.DataFrame({\n            'value': np.random.randn(n) * 100,\n            'category': np.random.choice(['A', 'B', 'C'], n)\n        })\n        \n        # Method 1: Boolean indexing (fastest)\n        start = time.time()\n        result1 = df[df['value'] > 50]\n        time1 = time.time() - start\n        \n        # Method 2: Query method (readable)\n        start = time.time()\n        result2 = df.query('value > 50')\n        time2 = time.time() - start\n        \n        # Method 3: Using apply (slowest)\n        start = time.time()\n        result3 = df[df.apply(lambda row: row['value'] > 50, axis=1)]\n        time3 = time.time() - start\n        \n        print(f\"Boolean indexing: {time1:.4f}s\")\n        print(f\"Query method: {time2:.4f}s\")\n        print(f\"Apply method: {time3:.4f}s\")\n        \n        return result1\n    \n    @staticmethod\n    def aggregation_comparison():\n        \"\"\"\n        Compare aggregation methods.\n        \"\"\"\n        # Create test data\n        df = pd.DataFrame({\n            'group': np.random.choice(['A', 'B', 'C'], 10000),\n            'value': np.random.randn(10000) * 100\n        })\n        \n        # Method 1: GroupBy (optimal)\n        start = time.time()\n        result1 = df.groupby('group')['value'].sum()\n        time1 = time.time() - start\n        \n        # Method 2: Pivot table\n        start = time.time()\n        result2 = df.pivot_table(values='value', index='group', aggfunc='sum')\n        time2 = time.time() - start\n        \n        # Method 3: Manual loop (terrible)\n        start = time.time()\n        result3 = {}\n        for group in df['group'].unique():\n            result3[group] = df[df['group'] == group]['value'].sum()\n        time3 = time.time() - start\n        \n        print(f\"GroupBy: {time1:.4f}s\")\n        print(f\"Pivot table: {time2:.4f}s\")\n        print(f\"Manual loop: {time3:.4f}s\")\n\n# ============================================================================\n# PART 4: QUICK ACCESS PATTERNS - Copy These During Interviews\n# ============================================================================\n\ndef quick_patterns():\n    \"\"\"\n    Quick copy-paste patterns for common interview questions.\n    Copy these directly during interviews and modify as needed.\n    \"\"\"\n    \n    # Sample DataFrame for testing\n    df = pd.DataFrame({\n        'category': ['A', 'B', 'A', 'B', 'C'] * 20,\n        'region': ['North', 'South', 'East', 'West', 'North'] * 20,\n        'value': np.random.randint(0, 100, 100),\n        'quantity': np.random.randint(1, 10, 100),\n        'date': pd.date_range('2024-01-01', periods=100, freq='D')\n    })\n    \n    # ========== PATTERN 1: Top N by Group ==========\n    top_3_per_region = (df\n                        .sort_values('value', ascending=False)\n                        .groupby('region')\n                        .head(3))\n    \n    # ========== PATTERN 2: GroupBy Multiple Aggregations ==========\n    summary = df.groupby('category').agg({\n        'value': ['sum', 'mean', 'count'],\n        'quantity': ['sum', 'mean']\n    }).round(2)\n    \n    # ========== PATTERN 3: Merge with Tracking ==========\n    df1 = df[['category', 'value']].head(50)\n    df2 = df[['category', 'quantity']].tail(50)\n    merged = pd.merge(df1, df2, on='category', how='outer', indicator=True)\n    \n    # ========== PATTERN 4: Handle Missing Values ==========\n    df_with_nulls = df.copy()\n    df_with_nulls.loc[::5, 'value'] = np.nan  # Add some nulls\n    df_clean = df_with_nulls.copy()\n    df_clean['value'].fillna(df_clean['value'].mean(), inplace=True)\n    \n    # ========== PATTERN 5: Remove Duplicates ==========\n    df_unique = df.drop_duplicates(subset=['category', 'region'], keep='first')\n    \n    # ========== PATTERN 6: Rolling Calculations ==========\n    df_sorted = df.sort_values('date')\n    df_sorted['rolling_mean_7'] = df_sorted['value'].rolling(window=7, min_periods=1).mean()\n    \n    # ========== PATTERN 7: Rank Within Groups ==========\n    df['rank'] = df.groupby('region')['value'].rank(method='dense', ascending=False)\n    top_3_ranked = df[df['rank'] <= 3]\n    \n    # ========== PATTERN 8: Cumulative Calculations ==========\n    df['cumulative_sum'] = df.groupby('category')['value'].cumsum()\n    df['pct_of_category'] = df.groupby('category')['value'].transform(lambda x: x / x.sum())\n    \n    # ========== PATTERN 9: Date Operations ==========\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['weekday'] = df['date'].dt.day_name()\n    monthly_avg = df.groupby(df['date'].dt.to_period('M'))['value'].mean()\n    \n    # ========== PATTERN 10: Complex Filtering ==========\n    filtered = df[(df['value'] > 50) & \n                  (df['category'].isin(['A', 'B'])) & \n                  (df['date'] >= '2024-02-01')]\n    \n    print(\"All quick patterns executed successfully!\")\n    return df\n\n# ============================================================================\n# PART 5: INTERVIEW PROBLEM SOLUTIONS - Complete Examples\n# ============================================================================\n\ndef interview_problem_1():\n    \"\"\"\n    PROBLEM: Find top 3 products by revenue in each region, \n    excluding products with less than 10 total sales.\n    \"\"\"\n    # Create sample data\n    np.random.seed(42)\n    sales = pd.DataFrame({\n        'product_id': np.random.choice(['P001', 'P002', 'P003', 'P004', 'P005'], 1000),\n        'region': np.random.choice(['North', 'South', 'East', 'West'], 1000),\n        'quantity': np.random.randint(1, 20, 1000),\n        'price': np.random.uniform(10, 100, 1000)\n    })\n    sales['revenue'] = sales['quantity'] * sales['price']\n    \n    # Solution\n    # Step 1: Find products with >= 10 total sales\n    product_sales = sales.groupby('product_id')['quantity'].sum()\n    valid_products = product_sales[product_sales >= 10].index\n    \n    # Step 2: Filter for valid products\n    valid_sales = sales[sales['product_id'].isin(valid_products)]\n    \n    # Step 3: Calculate revenue by region and product\n    revenue_by_region_product = (valid_sales\n                                 .groupby(['region', 'product_id'])['revenue']\n                                 .sum()\n                                 .reset_index())\n    \n    # Step 4: Get top 3 per region\n    top_products = (revenue_by_region_product\n                    .sort_values('revenue', ascending=False)\n                    .groupby('region')\n                    .head(3))\n    \n    return top_products\n\ndef interview_problem_2():\n    \"\"\"\n    PROBLEM: Calculate customer lifetime value (CLV) from transaction data.\n    Include: total spent, average order value, order count, days active.\n    \"\"\"\n    # Create sample data\n    transactions = pd.DataFrame({\n        'customer_id': np.random.choice(range(1, 101), 1000),\n        'order_date': pd.date_range('2023-01-01', periods=1000, freq='12H'),\n        'amount': np.random.uniform(10, 500, 1000)\n    })\n    \n    # Solution\n    clv = transactions.groupby('customer_id').agg({\n        'amount': ['sum', 'mean', 'count'],\n        'order_date': ['min', 'max']\n    })\n    \n    # Flatten column names\n    clv.columns = ['total_spent', 'avg_order_value', 'order_count', 'first_order', 'last_order']\n    \n    # Calculate days active\n    clv['days_active'] = (clv['last_order'] - clv['first_order']).dt.days + 1\n    \n    # Calculate order frequency\n    clv['orders_per_month'] = clv['order_count'] / (clv['days_active'] / 30)\n    \n    # Segment customers\n    clv['segment'] = pd.cut(clv['total_spent'], \n                            bins=[0, 100, 500, 1000, float('inf')],\n                            labels=['Low', 'Medium', 'High', 'VIP'])\n    \n    return clv.reset_index()\n\ndef interview_problem_3():\n    \"\"\"\n    PROBLEM: Detect potentially fraudulent transactions.\n    Flag if: amount > 3 std devs from mean, or > 5x user's average, \n    or multiple transactions within 1 minute.\n    \"\"\"\n    # Create sample data\n    transactions = pd.DataFrame({\n        'user_id': np.random.choice(range(1, 51), 500),\n        'timestamp': pd.date_range('2024-01-01', periods=500, freq='5T'),\n        'amount': np.random.exponential(50, 500)\n    })\n    # Add some suspicious transactions\n    transactions.loc[::50, 'amount'] *= 10  # Some very high amounts\n    \n    # Solution\n    # Calculate user statistics\n    user_stats = transactions.groupby('user_id')['amount'].agg(['mean', 'std']).reset_index()\n    \n    # Merge back to transactions\n    trans_with_stats = pd.merge(transactions, user_stats, on='user_id')\n    \n    # Flag 1: Amount > 3 std devs from overall mean\n    overall_mean = transactions['amount'].mean()\n    overall_std = transactions['amount'].std()\n    trans_with_stats['flag_outlier'] = (\n        trans_with_stats['amount'] > (overall_mean + 3 * overall_std)\n    )\n    \n    # Flag 2: Amount > 5x user's average\n    trans_with_stats['flag_user_unusual'] = (\n        trans_with_stats['amount'] > (trans_with_stats['mean'] * 5)\n    )\n    \n    # Flag 3: Multiple transactions within 1 minute\n    trans_sorted = trans_with_stats.sort_values(['user_id', 'timestamp'])\n    trans_sorted['time_diff'] = (trans_sorted\n                                 .groupby('user_id')['timestamp']\n                                 .diff()\n                                 .dt.total_seconds())\n    trans_sorted['flag_rapid'] = trans_sorted['time_diff'] < 60\n    \n    # Combine all flags\n    trans_sorted['is_suspicious'] = (\n        trans_sorted['flag_outlier'] | \n        trans_sorted['flag_user_unusual'] | \n        trans_sorted['flag_rapid']\n    )\n    \n    suspicious = trans_sorted[trans_sorted['is_suspicious']]\n    \n    return suspicious\n\n# ============================================================================\n# TESTING AND VALIDATION\n# ============================================================================\n\ndef test_all_patterns():\n    \"\"\"\n    Test that all pattern functions work correctly.\n    Run this to verify everything is working.\n    \"\"\"\n    print(\"Testing Core Patterns...\")\n    \n    # Create test DataFrame\n    test_df = pd.DataFrame({\n        'group': ['A', 'B', 'A', 'B', 'C'] * 20,\n        'category': ['X', 'Y', 'X', 'Y', 'Z'] * 20,\n        'value': np.random.randint(0, 100, 100),\n        'date_string': pd.date_range('2024-01-01', periods=100, freq='D').astype(str)\n    })\n    \n    # Test each core pattern\n    patterns = CorePatterns()\n    \n    try:\n        patterns.top_n_by_group(test_df, 'group', 'value', 3)\n        print(\"\u2713 Top N by Group\")\n        \n        patterns.groupby_multiple_agg(test_df)\n        print(\"\u2713 GroupBy Multiple Aggregations\")\n        \n        patterns.handle_missing_values(test_df)\n        print(\"\u2713 Handle Missing Values\")\n        \n        patterns.remove_duplicates_advanced(test_df)\n        print(\"\u2713 Remove Duplicates\")\n        \n        patterns.rolling_calculations(test_df)\n        print(\"\u2713 Rolling Calculations\")\n        \n        patterns.rank_within_groups(test_df)\n        print(\"\u2713 Rank Within Groups\")\n        \n        patterns.cumulative_calculations(test_df)\n        print(\"\u2713 Cumulative Calculations\")\n        \n    except Exception as e:\n        print(f\"\u2717 Error in patterns: {e}\")\n    \n    print(\"\\nTesting Gotcha Implementations...\")\n    \n    gotchas = GotchaImplementations()\n    test_list = [3, 1, 4, 1, 5, 9, 2, 6]\n    \n    try:\n        gotchas.bubble_sort(test_list)\n        print(\"\u2713 Bubble Sort\")\n        \n        gotchas.remove_duplicates_manual(test_list)\n        print(\"\u2713 Manual Deduplication\")\n        \n        gotchas.flatten_nested_manual([1, [2, 3], [4, [5, 6]]])\n        print(\"\u2713 Flatten Nested\")\n        \n        gotchas.count_occurrences_manual(test_list)\n        print(\"\u2713 Count Occurrences\")\n        \n    except Exception as e:\n        print(f\"\u2717 Error in gotchas: {e}\")\n    \n    print(\"\\nAll patterns tested successfully!\")\n\nif __name__ == \"__main__\":\n    # Run tests\n    test_all_patterns()\n    \n    # Show sample usage\n    print(\"\\n\" + \"=\"*50)\n    print(\"Pattern Library Ready!\")\n    print(\"=\"*50)\n    print(\"\\nUsage:\")\n    print(\"  from patterns_and_gotchas import CorePatterns, GotchaImplementations\")\n    print(\"  patterns = CorePatterns()\")\n    print(\"  result = patterns.top_n_by_group(df, 'category', 'value', 5)\")\n    print(\"\\nOr copy specific patterns from quick_patterns() during interviews\")\n",
  "docs/README.md": "# Python Analytics Interview Prep Platform\n\nA complete interview preparation platform with containerized development environment, smart content management, and structured learning path.\n\n## What You Get\n\n- **70 Flashcards** - Pandas/Python patterns for spaced repetition\n- **Structured Exercises** - Progressive difficulty with solutions\n- **Patterns Library** - Common interview patterns and gotchas\n- **Docker Environment** - Complete dev setup with Wing Pro IDE\n- **Smart Updates** - Preserves your work while updating content\n\n## Quick Start\n\n```bash\n# 1. Set up local environment\npython setup_python_interview_prep.py --mode local\n\n# 2. Start learning\ncd python-analytics-interview-prep\npython src/exercises.py\n\n# 3. (Optional) Build Docker environment\npython setup_python_interview_prep.py --mode docker\ndocker compose up\n# Access at http://localhost:6901\n```\n\n## Documentation\n\n- **[SETUP_GUIDE.md](SETUP_GUIDE.md)** - Complete installation and configuration\n- **[LEARNING_GUIDE.md](LEARNING_GUIDE.md)** - Study strategies and curriculum\n- **[PLATFORM_ARCHITECTURE.md](PLATFORM_ARCHITECTURE.md)** - Technical platform design\n\n## Requirements\n\n- Python 3.6+ \n- Docker Desktop (optional, for containerized environment)\n- 4GB disk space\n\n## Platform Structure\n\n```\npython-analytics-interview-prep/\n\u251c\u2500\u2500 src/              # Core Python modules\n\u251c\u2500\u2500 docs/             # Documentation\n\u251c\u2500\u2500 data/             # Flashcards and datasets  \n\u251c\u2500\u2500 docker/           # Container configuration\n\u251c\u2500\u2500 practice_work/    # Your solutions (preserved)\n\u2514\u2500\u2500 notes/           # Your notes (preserved)\n```\n\n## Study Path\n\n1. **Week 1**: Core patterns (groupby, merge, pivot)\n2. **Week 2**: Advanced patterns (window functions, optimization)\n3. **Week 3**: Practice problems and mock interviews\n4. **Week 4**: Review and weak area drilling\n\nSee [LEARNING_GUIDE.md](LEARNING_GUIDE.md) for detailed curriculum.\n\n## Contributing\n\nThis platform is designed for extensibility. Future modules planned:\n- SQL interview patterns\n- System design components\n- Behavioral interview guides\n- Company-specific preparation\n\n---\n\n*Built for real interview success, not just learning.*\n",
  "docs/SETUP_GUIDE.md": "# Setup Guide - Python Analytics Interview Prep Platform\n\nComplete guide for setting up the interview prep platform, including environment configuration, repository setup, and Docker containerization.\n\n## Prerequisites\n\n- Python 3.6+ installed\n- Docker Desktop (optional, for containerized mode)\n- Wing Pro IDE (optional, .deb file for Docker)\n- Windows/Mac/Linux command line access\n\n## One-Time Environment Setup\n\n### Windows PowerShell Configuration\n\n#### Step 1: Create PowerShell Profile\n```powershell\n# Check if profile exists\nTest-Path $PROFILE\n\n# If FALSE, create it\nNew-Item -Path $PROFILE -Type File -Force\n\n# Verify it was created\nTest-Path $PROFILE  # Should now show TRUE\n```\n\n#### Step 2: Add Environment Variables\n```powershell\n# Open profile in notepad\nnotepad $PROFILE\n\n# Add these exact lines:\n$env:PATTERNS_REPO = \"C:\\Users\\rayse\\Dropbox\\Projects\\GitHub\\data-engineering-patterns\"\n$env:PATH = \"$env:PATTERNS_REPO\\tools;$env:PATH\"\n\n# Save and close notepad\n```\n\n#### Step 3: Reload Profile\n```powershell\n# Reload the profile\n. $PROFILE\n\n# Verify environment variables are set\necho $env:PATTERNS_REPO\n# Should show: C:\\Users\\rayse\\Dropbox\\Projects\\GitHub\\data-engineering-patterns\n```\n\n#### Step 4: Test Path Configuration\n```powershell\n# From ANY directory, this should now work:\nsetup_python_interview_prep.py --help\n```\n\n### Mac/Linux Bash Configuration\n\n```bash\n# Mac/Linux (.bashrc or .zshrc)\nexport PATTERNS_REPO=\"/path/to/data-engineering-patterns\"\nexport PATH=\"$PATTERNS_REPO/tools:$PATH\"\n```\n\n### Download Wing Pro (Optional)\nFor Docker environment with Wing Pro:\n1. Go to https://wingware.com/downloads/wing-pro\n2. Download Ubuntu/Debian package (`.deb` file)\n3. Save to Downloads folder\n4. Generator will find it automatically\n\n## Installation\n\n### Get the Generator\nSave `setup_python_interview_prep.py` to:\n- `%PATTERNS_REPO%\\tools\\` (Windows)\n- `$PATTERNS_REPO/tools/` (Mac/Linux)\n\n### Three Modes of Operation\n\n#### Mode 1: Local Only (Default)\n```bash\n# Navigate to where you want the repo\ncd C:\\Users\\[username]\\Projects\n\n# Create/update local repository\nsetup_python_interview_prep.py --mode local\n\n# Result: ./python-analytics-interview-prep/ created\n```\n\n#### Mode 2: Docker Only\n```bash\n# Requires existing local repository\nsetup_python_interview_prep.py --mode docker\n\n# Builds Docker image with everything configured\n```\n\n#### Mode 3: All (Local + Docker)\n```bash\n# Does both in sequence\nsetup_python_interview_prep.py --mode all\n```\n\n## Repository Structure\n\nAfter setup, you'll have:\n```\npython-analytics-interview-prep/\n\u251c\u2500\u2500 src/              # Python source code\n\u2502   \u251c\u2500\u2500 exercises.py\n\u2502   \u2514\u2500\u2500 patterns_and_gotchas.py\n\u251c\u2500\u2500 docs/             # Documentation\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 SETUP_GUIDE.md\n\u2502   \u251c\u2500\u2500 LEARNING_GUIDE.md\n\u2502   \u2514\u2500\u2500 PLATFORM_ARCHITECTURE.md\n\u251c\u2500\u2500 data/             # Data files\n\u2502   \u2514\u2500\u2500 flashcards_complete.xlsx\n\u251c\u2500\u2500 docker/           # Docker configuration\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2514\u2500\u2500 wing-pro11_11.0.3.0_amd64.deb (if found)\n\u251c\u2500\u2500 practice_work/    # Your work (preserved)\n\u251c\u2500\u2500 notes/            # Your notes (preserved)\n\u2514\u2500\u2500 docker-compose.yml\n```\n\n## Docker Environment\n\n### What's Included\n- Ubuntu 22.04 with XFCE desktop\n- Python 3.13.6 (built from source)\n- Wing Pro 11 (if .deb found) or JupyterLab\n- All course materials mounted\n- VNC access via browser\n\n### Building Docker Image\n```bash\ncd python-analytics-interview-prep\ndocker compose build  # Takes 15-20 min first time\n```\n\n### Running Docker Container\n```bash\ndocker compose up\n\n# Access via browser\nhttp://localhost:6901\n# Password: student\n```\n\n### Managing Docker\n```bash\ndocker compose stop    # Stop container\ndocker compose start   # Restart container\ndocker compose down    # Remove container\ndocker compose logs    # View logs\n```\n\n## Testing Your Setup\n\n### Local Testing\n```bash\ncd python-analytics-interview-prep\npython src/exercises.py\n\n# Should see exercise descriptions\n# If import errors, check you're in root directory\n```\n\n### Docker Testing\n1. Open browser to http://localhost:6901\n2. Enter password: student\n3. Click Terminal icon on desktop\n4. Run:\n```bash\ncd ~/interview-prep\npython src/exercises.py\n```\n\n## Smart Features\n\n### Automatic Migration\n- Detects flat vs organized structure\n- Migrates files to proper directories\n- Preserves user work\n\n### Wing Detection\n- Searches Downloads folder\n- Searches script directory\n- Falls back to JupyterLab if not found\n\n### Update Preservation\nYour work in these directories is never overwritten:\n- `practice_work/`\n- `notes/`\n- `mock_interviews/`\n\n## Troubleshooting\n\n### Python Not Found\n```bash\n# Check Python version\npython --version\n\n# Should be 3.6+\n```\n\n### PowerShell Profile Issues\n```powershell\n# If \"cannot be loaded because running scripts is disabled\"\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n\n# If OneDrive sync issues with profile\n# Create profile in local directory instead:\n$LocalProfile = \"$env:USERPROFILE\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1\"\nNew-Item -Path $LocalProfile -Type File -Force\nnotepad $LocalProfile\n\n# Then create symbolic link to OneDrive location\nNew-Item -ItemType SymbolicLink -Path $PROFILE -Target $LocalProfile\n```\n\n### Docker Not Running\n- Install Docker Desktop from https://docker.com\n- Ensure Docker service is running\n- Check whale icon in system tray\n\n### Port Already in Use\nEdit `docker-compose.yml`:\n```yaml\nports:\n  - \"6902:6901\"  # Change to different port\n```\n\n### Wing License Issues\n- Mount your license file in docker-compose.yml\n- Or use Wing Pro trial (30 days)\n- Or use JupyterLab (free alternative)\n\n### Import Errors\nAlways run from repository root:\n```bash\n# Correct\ncd python-analytics-interview-prep\npython src/exercises.py\n\n# Wrong\ncd src\npython exercises.py\n```\n\n## Workflow Examples\n\n### Monday: Fresh Setup\n```bash\n# Download Wing to Downloads\n# Navigate to GitHub directory\ncd C:\\Users\\[username]\\GitHub\n\n# Set up local repository\nsetup_python_interview_prep.py --mode local\n\n# Test it works\ncd python-analytics-interview-prep\npython src/exercises.py\n```\n\n### Tuesday: Add Docker\n```bash\n# After local testing successful\nsetup_python_interview_prep.py --mode docker\n\n# Launch containerized environment\ndocker compose up\n```\n\n### Future Updates\n```bash\n# Regenerate with latest content\nsetup_python_interview_prep.py --mode local\n\n# Your work is preserved, content updated\n```\n\n## Platform Extensions\n\nThe platform supports future modules:\n- Additional flashcard sets\n- New exercise patterns\n- SQL interview prep\n- System design guides\n\nSimply add new content to appropriate directories.\n\n---\n\nFor technical architecture details, see [PLATFORM_ARCHITECTURE.md](PLATFORM_ARCHITECTURE.md)\n",
  "docs/LEARNING_GUIDE.md": "# Learning Guide - Python Analytics Interview Prep\n\nComprehensive guide for studying effectively with the interview prep platform.\n\n## Learning Philosophy\n\nThis platform uses **active learning** with immediate application:\n1. **Learn** - Read concept (20%)\n2. **Practice** - Code solution (50%)\n3. **Retain** - Flashcard drilling (30%)\n\n## 28-Day Curriculum Overview\n\n### Week 1: Foundations (Days 1-7)\n- DataFrame basics and operations\n- GroupBy patterns\n- Merge/join operations\n- Data cleaning techniques\n\n### Week 2: Advanced Patterns (Days 8-14)\n- Window functions\n- Pivot operations\n- Performance optimization\n- Complex aggregations\n\n### Week 3: Problem Solving (Days 15-21)\n- Multi-step problems\n- Edge case handling\n- Time/space optimization\n- Debugging strategies\n\n### Week 4: Interview Ready (Days 22-28)\n- Mock interviews\n- Time pressure practice\n- Communication skills\n- Company-specific prep\n\n## Daily Study Schedule (90-120 minutes)\n\n### Morning Session (30 min)\n- **10 min**: Review flashcards (spaced repetition)\n- **20 min**: Read new concepts in course_with_schedule.md\n\n### Main Session (60-90 min)\n- **30 min**: Complete daily exercises\n- **20 min**: Review and understand solutions\n- **20 min**: Practice variations\n- **20 min**: Code from memory\n\n### Evening Review (Optional, 30 min)\n- Update notes on challenging concepts\n- Extra flashcard drilling for weak areas\n- Prepare questions for next day\n\n## Using the Materials\n\n### 1. Course Content (`docs/course_with_schedule.md`)\n```python\n# Day 1 Example Flow\n1. Read concept explanation\n2. Study the example code\n3. Try exercise without looking\n4. Check solution\n5. Implement variation\n```\n\n### 2. Exercises (`src/exercises.py`)\n```python\n# Progressive difficulty\nexercise_1_1()  # Basic - Learn syntax\nexercise_1_2()  # Medium - Apply concept\nexercise_1_3()  # Hard - Combine concepts\n\n# Test your solution\nfrom exercises import sample_df\nresult = your_solution(sample_df)\nprint(result)\n```\n\n### 3. Flashcards (`data/flashcards_complete.xlsx`)\n- **Upload to Cram.com** for best experience\n- **Categories**: Focus on weak areas\n- **Frequency**: 2x daily minimum\n- **Target**: 95% recall before interviews\n\n### 4. Patterns Reference (`src/patterns_and_gotchas.py`)\n```python\n# Quick lookup when stuck\nfrom patterns_and_gotchas import groupby_patterns\nhelp(groupby_patterns)\n\n# Common gotchas to memorize\n- reset_index() after groupby\n- merge vs join differences\n- copy() vs view behavior\n```\n\n## Practice Strategies\n\n### Active Coding\n```python\n# Don't copy-paste! Type everything\n# practice_work/day01.py\ndef my_groupby_solution(df):\n    # Type from memory first\n    # Then check solution\n    return df.groupby('category').agg({'value': 'sum'})\n```\n\n### Progressive Difficulty\n1. **Solve with hints** (look at pattern library)\n2. **Solve without hints** (only documentation)\n3. **Solve under time pressure** (15 min limit)\n4. **Solve while explaining** (mock interview)\n\n### Error Learning\nKeep an error log:\n```python\n# notes/errors.md\n## Day 3: KeyError\nProblem: df['column'] failed\nSolution: Check df.columns first\nLearning: Always validate assumptions\n```\n\n## Communication Practice\n\n### The STAR Method\n- **Situation**: \"Given a DataFrame with...\"\n- **Task**: \"I need to find...\"\n- **Action**: \"I'll use groupby because...\"\n- **Result**: \"This gives us...\"\n\n### Think Aloud Protocol\n```python\n# Practice narrating your thought process\n\"First, I'll examine the data structure...\"\n\"I notice there might be duplicates, so...\"\n\"Let me verify my assumption by...\"\n\"The time complexity would be...\"\n```\n\n## Mock Interview Schedule\n\n### Week 1: Foundation Check\n- 10 min: Basic DataFrame operations\n- Focus: Correct syntax\n\n### Week 2: Concept Application\n- 20 min: Medium difficulty problem\n- Focus: Choosing right approach\n\n### Week 3: Complex Problems\n- 30 min: Multi-step solution\n- Focus: Breaking down problem\n\n### Week 4: Full Simulation\n- 45 min: Complete interview\n- Focus: Communication + code\n\n## Success Metrics\n\n### Daily\n- \u2705 Complete all exercises\n- \u2705 <5 min for basic problems\n- \u2705 Flashcard streak maintained\n\n### Weekly\n- \u2705 Mock interview improvement\n- \u2705 Solve new problems without help\n- \u2705 Explain solutions clearly\n\n### Pre-Interview\n- \u2705 95% flashcard recall\n- \u2705 Solve medium problems in <10 min\n- \u2705 Comfortable with edge cases\n\n## Common Pitfalls to Avoid\n\n### 1. Passive Reading\n\u274c Just reading solutions\n\u2705 Code everything yourself\n\n### 2. Skipping Foundations\n\u274c Jumping to hard problems\n\u2705 Master basics first\n\n### 3. Not Timing Yourself\n\u274c Taking unlimited time\n\u2705 Set timer for each problem\n\n### 4. Silent Coding\n\u274c Coding without explaining\n\u2705 Always narrate approach\n\n## Weak Area Diagnosis\n\n### Can't Start?\n- Review patterns library\n- Break problem into steps\n- Start with data exploration\n\n### Syntax Errors?\n- Drill flashcards more\n- Use quick_reference.md\n- Type more, copy less\n\n### Logic Errors?\n- Draw data transformations\n- Test with simple examples\n- Use print debugging\n\n### Too Slow?\n- Practice common patterns\n- Learn keyboard shortcuts\n- Prepare code templates\n\n## Study Tools Integration\n\n### Wing IDE Setup\n- Set breakpoints for debugging\n- Use interactive console\n- Configure test runner\n\n### Jupyter Alternative\n```bash\n# If using JupyterLab in Docker\njupyter lab --notebook-dir=/home/student/interview-prep\n```\n\n### Version Control\n```bash\n# Track your progress\ngit init\ngit add practice_work/\ngit commit -m \"Day 1 solutions\"\n```\n\n## Pre-Interview Checklist\n\n### 1 Week Before\n- [ ] Complete all 31 exercises\n- [ ] 95% flashcard accuracy\n- [ ] 3 mock interviews done\n- [ ] Patterns memorized\n\n### 1 Day Before\n- [ ] Review error log\n- [ ] Quick flashcard review\n- [ ] Test environment setup\n- [ ] Prepare questions for interviewer\n\n### Day Of\n- [ ] Warm up with easy problem\n- [ ] Review communication tips\n- [ ] Test audio/video/screen share\n- [ ] Have water ready\n\n## Continuous Improvement\n\n### After Each Session\n1. What was challenging?\n2. What pattern helped most?\n3. What would I do differently?\n\n### After Each Week\n1. Which concepts need review?\n2. Are flashcards effective?\n3. Is pace sustainable?\n\n### After Mock Interviews\n1. Where did I struggle?\n2. Was explanation clear?\n3. Did I manage time well?\n\n---\n\nRemember: **Consistency > Intensity**. Daily practice beats weekend cramming every time.\n\nFor technical details about the platform, see [PLATFORM_ARCHITECTURE.md](PLATFORM_ARCHITECTURE.md)\n",
  "docs/PLATFORM_ARCHITECTURE.md": "# Platform Architecture - Python Analytics Interview Prep\n\nTechnical architecture of the interview prep platform, describing the engine, modules, and extensibility design.\n\n## Platform Vision\n\nThis is not just a course, but a **learning platform** with:\n- **Core Engine**: Smart generator with merge capabilities\n- **Pluggable Modules**: Interview prep content (Python, SQL, System Design)\n- **Integrations**: Cram.com, Docker, Wing IDE\n- **Extensibility**: Add new courses without changing engine\n\n## System Overview\n\nThis document describes the technical architecture of the interview prep deployment system, specifically the consolidator approach for maintaining and distributing the learning materials.\n\n## The Problem\n\n- Multiple learning artifacts need to be deployed as a cohesive system\n- User needs simple, one-command deployment\n- Changes need to flow bidirectionally (Claude \u2194 User)\n- No direct write access from Claude to user's filesystem/GitHub\n\n## The Solution: Consolidator Architecture\n\n### Core Concept\n\nA consolidator script reads all artifacts and generates a single, self-contained setup script that the user downloads and runs.\n\n```\n[Artifacts] \u2192 [Consolidator] \u2192 [setup_complete.py] \u2192 [User Repo]\n```\n\n### Why This Approach\n\n1. **Single Download**: User gets one file, not multiple\n2. **Self-Contained**: No external dependencies or files to fetch\n3. **Version Controlled**: Each generated script is a complete snapshot\n4. **Simple Update**: Re-run consolidator for new version\n5. **No Manual Steps**: Everything automated\n\n## File Structure\n\n### Claude's Working Directory (`/mnt/user-data/outputs/`)\n```\n/mnt/user-data/outputs/\n\u251c\u2500\u2500 course_with_schedule.md      # Teaching content\n\u251c\u2500\u2500 exercises.py                  # Practice problems\n\u251c\u2500\u2500 flashcards_complete.xlsx      # Spaced repetition (binary)\n\u251c\u2500\u2500 patterns_and_gotchas.py       # Reference implementations\n\u251c\u2500\u2500 talking_points.md              # Communication guide\n\u251c\u2500\u2500 quick_reference.md             # Syntax lookup\n\u251c\u2500\u2500 README.md                      # Usage instructions\n\u251c\u2500\u2500 PROJECT_STATUS.md              # AI continuity\n\u251c\u2500\u2500 SETUP.md                       # Deployment guide\n\u251c\u2500\u2500 ARCHITECTURE.md                # This file\n\u251c\u2500\u2500 consolidator.py                # Generates setup_complete.py\n\u2514\u2500\u2500 setup_complete.py              # Generated output (10K+ lines)\n```\n\n### User's Repository (After Running Setup)\n```\npython-analytics-interview-prep/\n\u251c\u2500\u2500 course_with_schedule.md\n\u251c\u2500\u2500 exercises.py\n\u251c\u2500\u2500 flashcards_complete.xlsx\n\u251c\u2500\u2500 patterns_and_gotchas.py\n\u251c\u2500\u2500 talking_points.md\n\u251c\u2500\u2500 quick_reference.md\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 PROJECT_STATUS.md\n\u251c\u2500\u2500 SETUP.md\n\u251c\u2500\u2500 ARCHITECTURE.md\n\u251c\u2500\u2500 practice_work/                # User's work\n\u251c\u2500\u2500 mock_interviews/              # Practice problems\n\u251c\u2500\u2500 notes/                        # Personal notes\n\u2514\u2500\u2500 archive/                      # Original patterns\n    \u2514\u2500\u2500 patterns/                 # Preserved for review\n```\n\n## The Consolidator Script\n\n### Design Principles\n\n1. **Read Once**: Load all artifacts into memory\n2. **Embed Fully**: Include complete content, not references\n3. **Handle Binary**: Special handling for Excel files (base64 encoded)\n4. **Create Structure**: Generate directory tree\n5. **Be Idempotent**: Running twice produces same result\n\n### Pseudo-Implementation\n\n```python\n# consolidator.py structure (simplified)\n\ndef consolidate():\n    \"\"\"Generate setup_complete.py with all content embedded\"\"\"\n    \n    # 1. Read all text artifacts\n    artifacts = {}\n    for file in text_files:\n        artifacts[file] = read_file_content(file)\n    \n    # 2. Handle binary files (Excel)\n    artifacts['flashcards.xlsx'] = base64_encode(read_binary(file))\n    \n    # 3. Generate setup script\n    setup_script = f'''\n#!/usr/bin/env python3\n\"\"\"\nAuto-generated setup script\nContains all interview prep materials\nGenerated: {datetime.now()}\n\"\"\"\n\nARTIFACTS = {{\n    'course_with_schedule.md': '''{artifacts['course']}''',\n    'exercises.py': '''{artifacts['exercises']}''',\n    # ... all other artifacts\n}}\n\ndef create_repo():\n    for filename, content in ARTIFACTS.items():\n        write_file(filename, content)\n    create_directories(['practice_work', 'notes', 'archive'])\n    print(\"Setup complete!\")\n\nif __name__ == \"__main__\":\n    create_repo()\n'''\n    \n    # 4. Write setup_complete.py\n    write_file('setup_complete.py', setup_script)\n```\n\n## Bidirectional Sync Workflow\n\n### Claude \u2192 User (Deployment)\n\n```mermaid\ngraph LR\n    A[Claude edits artifacts] --> B[Run consolidator.py]\n    B --> C[Generate setup_complete.py]\n    C --> D[User downloads]\n    D --> E[User runs script]\n    E --> F[Repo created/updated]\n```\n\n### User \u2192 Claude (Updates)\n\n```mermaid\ngraph LR\n    A[User edits in Wing] --> B[Commit to GitHub]\n    B --> C[Refresh KB in Claude]\n    C --> D[Claude sees changes]\n    D --> E[Update artifacts]\n    E --> F[Regenerate setup]\n```\n\n## Key Design Decisions\n\n### Why Embed Everything\n\n**Alternative Considered**: Modular files with references\n**Decision**: Embed all content directly\n**Rationale**: \n- Single file download is simpler\n- No dependency management\n- No network requests during setup\n- File size (10K lines) is manageable for modern systems\n\n### Why Not Use Git Directly\n\n**Alternative Considered**: Claude pushes to GitHub\n**Decision**: Generate downloadable script\n**Rationale**:\n- Claude lacks GitHub write access (current limitation)\n- Security model clearer with explicit download\n- User maintains control of their repository\n- Future upgrade path when Claude Code Git integration matures\n\n### Why Archive Old Patterns\n\n**Alternative Considered**: Delete old pattern library\n**Decision**: Preserve in `/archive` folder\n**Rationale**:\n- Patterns may contain useful examples\n- User can cherry-pick valuable content\n- No destructive operations\n- Preserves work history\n\n## Deployment Workflow\n\n### Setup Location\n\nThe generator script lives in your main patterns repository:\n```\ndata-engineering-patterns/\n\u2514\u2500\u2500 tools/\n    \u2514\u2500\u2500 setup_python_interview_prep.py\n```\n\nThe interview prep repository is created separately:\n```\nGitHub/\n\u251c\u2500\u2500 data-engineering-patterns/         # Your real patterns (has the generator)\n\u2514\u2500\u2500 python-analytics-interview-prep/   # Created by generator (study materials)\n```\n\n### Running the Generator\n\n1. **Navigate to your GitHub directory:**\n```bash\ncd C:\\Users\\rayse\\Dropbox\\Projects\\GitHub\n```\n\n2. **Run the generator from tools:**\n```bash\npython data-engineering-patterns\\tools\\setup_python_interview_prep.py\n```\n\n3. **Result:**\nCreates or updates `python-analytics-interview-prep/` in current directory\n\n### Important Notes\n\n- **Always run from GitHub directory**, not from tools folder\n- Script creates subfolder in current directory\n- Safe to run multiple times (updates materials, preserves your work)\n- Your practice work in `practice_work/`, `notes/`, etc. is never overwritten\n\n### Full Workflow Example\n\n```bash\n# First time setup\ncd C:\\Users\\rayse\\Dropbox\\Projects\\GitHub\npython data-engineering-patterns\\tools\\setup_python_interview_prep.py\ncd python-analytics-interview-prep\npip install -r requirements.txt\n\n# Getting updates from Claude\n# 1. Claude regenerates setup_python_interview_prep.py\n# 2. You download and copy to tools folder\n# 3. Run again:\ncd C:\\Users\\rayse\\Dropbox\\Projects\\GitHub\npython data-engineering-patterns\\tools\\setup_python_interview_prep.py\n# Updates materials, preserves your work\n```\n\n## Binary File Handling\n\n### The Challenge\nExcel files (flashcards) can't be embedded as text\n\n### The Solution\nBase64 encoding in setup script:\n```python\nFLASHCARDS_B64 = \"\"\"[base64 encoded content]\"\"\"\nwith open('flashcards_complete.xlsx', 'wb') as f:\n    f.write(base64.b64decode(FLASHCARDS_B64))\n```\n\n## Future Enhancements\n\n### When Claude Code Matures\n- Direct Git push from Claude\n- Eliminate download step\n- Real-time sync\n\n### Potential Optimizations\n- Incremental updates (diff-based)\n- Compressed encoding for smaller file size\n- Web-based installer instead of Python script\n\n### Extended Features\n- Multi-version management\n- Rollback capability\n- Update notifications\n- Progress tracking integration\n\n## Error Handling Strategy\n\n### In Consolidator\n- Verify all artifacts exist before generating\n- Validate content encoding\n- Check file size limits\n\n### In Generated Setup\n- Check Python version\n- Verify write permissions\n- Handle existing files gracefully\n- Provide clear error messages\n\n## Security Considerations\n\n### What's Safe\n- Script only writes to local directory\n- No network operations\n- No system modifications\n- User controls execution\n\n### What to Watch\n- Large file size (10K+ lines)\n- Binary content embedding\n- Overwriting existing files\n\n## Testing Strategy\n\n### Consolidator Testing\n1. Verify all artifacts included\n2. Check encoding/decoding cycle\n3. Test with missing files\n4. Validate generated Python syntax\n\n### Setup Script Testing\n1. Fresh installation\n2. Update over existing\n3. Permission edge cases\n4. Cross-platform compatibility\n\n## Maintenance Notes\n\n### Adding New Artifacts\n1. Create artifact in `/mnt/user-data/outputs/`\n2. Update consolidator.py to include it\n3. Regenerate setup_complete.py\n4. Update README with new artifact description\n\n### Modifying Existing Artifacts\n1. Edit in place\n2. Re-run consolidator\n3. New setup_complete.py has updates\n4. User runs to update their copy\n\n## Performance Characteristics\n\n### File Sizes (Approximate)\n- Consolidator.py: ~500 lines\n- Setup_complete.py: ~10,000 lines\n- Deployed repo: ~2 MB total\n- Execution time: < 5 seconds\n\n### Scaling Limits\n- Text files: No practical limit\n- Binary files: Base64 increases size by ~33%\n- Total script size: Python can handle 100K+ lines\n\n## Conclusion\n\nThe consolidator architecture provides a pragmatic solution to the deployment challenge, working within Claude's current limitations while maintaining full automation. It's not the most elegant solution, but it's simple, reliable, and achieves the goal of one-click deployment.\n\nWhen Claude's Git integration improves, this architecture can be easily replaced with direct repository writes. Until then, this approach delivers a working solution today.\n\n---\n\n*Architecture Version: 1.0*\n*Last Updated: January 2025*",
  "docs/course_with_schedule.md": "# Python Analytics Engineering Interview Prep Course\n\n## Course Overview\n\nThis comprehensive course teaches Python from fundamentals through interview-ready patterns in 21 days. Each module includes concept explanations, code examples, practice exercises, and integration with flashcards.\n\n**Structure:**\n- 6 Learning Modules (concepts with examples)\n- 60 Progressive Exercises (10 per module)\n- 70 Flashcards (aligned with modules)\n- 21-Day Study Schedule (integrated throughout)\n\n**Daily Commitment:** 90-120 minutes\n- Morning (30 min): Learn concepts\n- Afternoon (30 min): Complete exercises\n- Evening (30-60 min): Flashcard review\n\n---\n\n# PART 1: PYTHON FOUNDATIONS\n\n## Module 1: Python Data Structures & Operations\n\n### Day 1-2 Schedule\n- **Day 1:** Lists and list operations (1.1-1.2)\n- **Day 2:** Dictionaries and sets (1.3-1.4)\n- **Exercises:** Complete Module 1 exercises\n- **Flashcards:** Focus on \"Syntax Essentials\" cards for data structures\n\n### 1.1 Lists and List Operations\n\nLists are ordered, mutable collections - the foundation of data manipulation in Python.\n\n```python\n# Creating and accessing lists\nnumbers = [1, 2, 3, 4, 5]\nmixed = [1, \"hello\", 3.14, True]\n\n# Indexing and slicing\nfirst = numbers[0]         # 1\nlast = numbers[-1]         # 5\nsubset = numbers[1:3]      # [2, 3]\nreversed = numbers[::-1]   # [5, 4, 3, 2, 1]\n\n# Essential list methods\nnumbers.append(6)          # Add to end: [1, 2, 3, 4, 5, 6]\nnumbers.extend([7, 8])     # Add multiple: [1, 2, 3, 4, 5, 6, 7, 8]\nnumbers.insert(0, 0)       # Insert at position\nnumbers.remove(3)          # Remove first occurrence of value\npopped = numbers.pop()     # Remove and return last item\nnumbers.clear()            # Remove all items\n\n# Common list operations\nlength = len(numbers)\ntotal = sum(numbers)\nminimum = min(numbers)\nmaximum = max(numbers)\nsorted_list = sorted(numbers)  # Returns new sorted list\nnumbers.sort()                  # Sorts in place\n```\n\n**Interview Focus:** Lists are used in nearly every problem. Master slicing syntax and understand mutable vs immutable operations.\n\n### 1.2 List Pitfalls and Best Practices\n\n```python\n# PITFALL: Modifying list while iterating\nnumbers = [1, 2, 3, 4, 5]\n# WRONG - will skip elements:\nfor num in numbers:\n    if num % 2 == 0:\n        numbers.remove(num)  # DON'T DO THIS\n\n# RIGHT - use list comprehension:\nnumbers = [num for num in numbers if num % 2 != 0]\n\n# PITFALL: Shallow vs deep copy\noriginal = [[1, 2], [3, 4]]\nshallow_copy = original.copy()  # or original[:]\nimport copy\ndeep_copy = copy.deepcopy(original)\n\noriginal[0][0] = 999\n# shallow_copy[0][0] is now 999 too!\n# deep_copy[0][0] is still 1\n\n# BEST PRACTICE: List comprehension for transformation\n# Instead of:\nsquares = []\nfor x in range(10):\n    squares.append(x**2)\n\n# Use:\nsquares = [x**2 for x in range(10)]\n```\n\n### 1.3 Dictionaries\n\nDictionaries are key-value pairs - essential for grouping and counting operations.\n\n```python\n# Creating dictionaries\nperson = {'name': 'Alice', 'age': 30, 'city': 'NYC'}\nword_counts = {}  # Empty dict for counting pattern\n\n# Accessing values\nname = person['name']                    # KeyError if missing\nage = person.get('age', 0)              # Safe with default\nperson['email'] = 'alice@example.com'   # Add/update\n\n# Dictionary methods\nkeys = person.keys()      # dict_keys(['name', 'age', 'city', 'email'])\nvalues = person.values()  # dict_values(['Alice', 30, 'NYC', 'alice@example.com'])\nitems = person.items()    # dict_items([('name', 'Alice'), ...])\n\n# Common patterns\n# Counting pattern (crucial for interviews)\ncounts = {}\nfor item in items_list:\n    counts[item] = counts.get(item, 0) + 1\n\n# Or using defaultdict\nfrom collections import defaultdict\ncounts = defaultdict(int)\nfor item in items_list:\n    counts[item] += 1\n\n# Grouping pattern\ngroups = defaultdict(list)\nfor record in records:\n    key = record['category']\n    groups[key].append(record)\n\n# Dictionary comprehension\nsquares = {x: x**2 for x in range(5)}  # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n```\n\n### 1.4 Sets\n\nSets contain unique, unordered values - perfect for deduplication and membership testing.\n\n```python\n# Creating sets\nunique_nums = {1, 2, 3, 3, 4}  # {1, 2, 3, 4} - duplicates removed\nempty_set = set()  # Note: {} creates empty dict, not set\n\n# Set operations (interview favorites)\nset1 = {1, 2, 3}\nset2 = {2, 3, 4}\n\nintersection = set1 & set2    # {2, 3} - in both\nunion = set1 | set2          # {1, 2, 3, 4} - in either\ndifference = set1 - set2     # {1} - in set1 but not set2\nsymmetric_diff = set1 ^ set2  # {1, 4} - in either but not both\n\n# Fast membership testing\n# O(1) average case vs O(n) for lists\nmillions = set(range(1000000))\nif 500000 in millions:  # Lightning fast\n    print(\"Found!\")\n\n# Common interview pattern: deduplication\ndef remove_duplicates(items):\n    return list(set(items))  # Note: loses order\n\n# Preserve order while removing duplicates:\ndef remove_duplicates_ordered(items):\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n```\n\n### Module 1 Exercises\n\n**Exercise 1.1 (Easy):** Create a list of numbers 1-10 and return only even numbers.\n**Exercise 1.2 (Medium):** Given a list with duplicates `[1, 2, 2, 3, 3, 3, 4]`, return unique values in original order.\n**Exercise 1.3 (Hard):** Merge two dictionaries, but if keys overlap, keep the higher value.\n**Exercise 1.4 (Debug):** Fix this code that should remove duplicates.\n**Exercise 1.5 (Interview):** Count frequency of words in a sentence using a dictionary.\n\n---\n\n## Module 2: List Comprehensions & String Operations\n\n### Day 3 Schedule\n- **Morning:** List comprehensions (2.1)\n- **Afternoon:** String operations (2.2)\n- **Exercises:** Complete Module 2 exercises\n- **Flashcards:** Add comprehension patterns\n\n### 2.1 List Comprehensions\n\nList comprehensions are Python's most powerful pattern for data transformation - master these for interviews.\n\n```python\n# Basic comprehension structure\n# [expression for item in iterable if condition]\n\n# Traditional loop approach:\nsquares = []\nfor x in range(10):\n    if x % 2 == 0:\n        squares.append(x**2)\n\n# Comprehension approach (preferred):\nsquares = [x**2 for x in range(10) if x % 2 == 0]\n\n# Common patterns you'll use constantly:\n# 1. Filter and transform\nnumbers = [1, 2, 3, 4, 5]\ndoubled_evens = [x * 2 for x in numbers if x % 2 == 0]  # [4, 8]\n\n# 2. Flatten nested structure\nmatrix = [[1, 2], [3, 4], [5, 6]]\nflat = [item for sublist in matrix for item in sublist]  # [1, 2, 3, 4, 5, 6]\n\n# 3. Conditional expression (ternary)\nlabels = ['even' if x % 2 == 0 else 'odd' for x in range(5)]\n# ['even', 'odd', 'even', 'odd', 'even']\n\n# 4. Multiple conditions\nfiltered = [x for x in range(20) if x % 2 == 0 if x % 3 == 0]  # [0, 6, 12, 18]\n\n# Dictionary comprehension\nword = \"hello\"\nchar_counts = {char: word.count(char) for char in set(word)}\n# {'h': 1, 'e': 1, 'l': 2, 'o': 1}\n\n# Set comprehension\nsquares_set = {x**2 for x in range(5)}  # {0, 1, 4, 9, 16}\n\n# Nested comprehensions (be careful with readability)\nmultiplication_table = [[i * j for j in range(1, 4)] for i in range(1, 4)]\n# [[1, 2, 3], [2, 4, 6], [3, 6, 9]]\n```\n\n**Interview Tip:** Comprehensions show Python fluency. Use them for simple transformations, but don't sacrifice readability for cleverness.\n\n### 2.2 String Operations\n\nString manipulation is crucial for data cleaning and text processing tasks.\n\n```python\n# Essential string methods\ntext = \"  Data Engineering with Python  \"\n\n# Cleaning operations\ntext.strip()                      # Remove whitespace: \"Data Engineering with Python\"\ntext.lower()                      # Lowercase: \"  data engineering with python  \"\ntext.upper()                      # Uppercase: \"  DATA ENGINEERING WITH PYTHON  \"\ntext.replace('Python', 'Pandas')  # Replace: \"  Data Engineering with Pandas  \"\n\n# Splitting and joining\nwords = text.strip().split()      # ['Data', 'Engineering', 'with', 'Python']\n' '.join(words)                   # \"Data Engineering with Python\"\n'-'.join(words)                   # \"Data-Engineering-with-Python\"\n\n# Checking content\ntext.startswith('  Data')         # True\ntext.endswith('Python  ')         # True\n'Engineering' in text              # True\ntext.count('ing')                  # 2\n\n# String formatting (modern Python)\nname = \"Alice\"\nage = 30\n# f-strings (preferred)\nmessage = f\"{name} is {age} years old\"\n# Format method\nmessage = \"{} is {} years old\".format(name, age)\n# Percentage (old style - avoid)\nmessage = \"%s is %d years old\" % (name, age)\n\n# Common interview patterns\n# 1. Clean messy data\ndef clean_string(s):\n    return s.strip().lower().replace(' ', '_')\n\n# 2. Extract information\nemail = \"user@example.com\"\nusername = email.split('@')[0]  # \"user\"\ndomain = email.split('@')[1]    # \"example.com\"\n\n# 3. Validate format\ndef is_valid_email(email):\n    return '@' in email and '.' in email.split('@')[1]\n```\n\n### Module 2 Exercises\n\n**Exercise 2.1 (Easy):** Use list comprehension to get squares of only positive numbers from `[-2, -1, 0, 1, 2, 3]`.\n**Exercise 2.2 (Medium):** Given list of names, create dictionary with name as key and name length as value.\n**Exercise 2.3 (Hard):** Flatten this nested structure: `[1, [2, 3], [4, [5, 6]], 7]` to `[1, 2, 3, 4, 5, 6, 7]`.\n**Exercise 2.4 (Debug):** Fix the comprehension that should get words longer than 3 characters.\n**Exercise 2.5 (Interview):** Clean email addresses: lowercase, remove spaces, validate format.\n\n---\n\n## Module 3: Functions & Lambda\n\n### Day 4 Schedule\n- **Morning:** Functions with default arguments (3.1)\n- **Afternoon:** Lambda functions and functional programming (3.2)\n- **Exercises:** Complete Module 3 exercises\n- **Flashcards:** Lambda and function patterns\n\n### 3.1 Functions with Default Arguments\n\nFunctions are the building blocks of clean, reusable code. Master default arguments and parameter handling.\n\n```python\n# Basic function structure\ndef process_data(data, method='mean', handle_nulls=True):\n    \"\"\"\n    Process data with configurable options.\n    \n    Args:\n        data: Input data to process\n        method: Aggregation method ('mean', 'sum', 'median')\n        handle_nulls: Whether to remove null values first\n    \n    Returns:\n        Processed result\n    \"\"\"\n    if handle_nulls and hasattr(data, 'dropna'):\n        data = data.dropna()\n    \n    if method == 'mean':\n        return data.mean() if hasattr(data, 'mean') else sum(data) / len(data)\n    elif method == 'sum':\n        return data.sum() if hasattr(data, 'sum') else sum(data)\n    elif method == 'median':\n        sorted_data = sorted(data)\n        n = len(sorted_data)\n        return sorted_data[n // 2] if n % 2 else (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2\n    \n    return data\n\n# CRITICAL PITFALL: Mutable default arguments\n# WRONG - default list is shared across calls:\ndef add_to_list(item, target=[]):  # DON'T DO THIS!\n    target.append(item)\n    return target\n\nresult1 = add_to_list(1)  # [1]\nresult2 = add_to_list(2)  # [1, 2] - UNEXPECTED!\n\n# RIGHT - use None as default:\ndef add_to_list(item, target=None):\n    if target is None:\n        target = []\n    target.append(item)\n    return target\n\n# Variable arguments\ndef process_multiple(*args, **kwargs):\n    \"\"\"\n    *args: captures positional arguments as tuple\n    **kwargs: captures keyword arguments as dict\n    \"\"\"\n    print(f\"Positional: {args}\")\n    print(f\"Keyword: {kwargs}\")\n\nprocess_multiple(1, 2, 3, name='Alice', age=30)\n# Positional: (1, 2, 3)\n# Keyword: {'name': 'Alice', 'age': 30}\n\n# Unpacking arguments\nvalues = [1, 2, 3]\nresult = sum(*values)  # Unpacks list to arguments\n\nconfig = {'method': 'mean', 'handle_nulls': False}\nresult = process_data(data, **config)  # Unpacks dict to kwargs\n```\n\n### 3.2 Lambda Functions and Functional Programming\n\nLambda functions are anonymous functions used for simple operations - extremely common in pandas operations.\n\n```python\n# Lambda syntax: lambda arguments: expression\nsquare = lambda x: x**2\nadd = lambda x, y: x + y\n\n# Most common use: with sorted()\ndata = [{'name': 'Alice', 'age': 30}, {'name': 'Bob', 'age': 25}]\nsorted_by_age = sorted(data, key=lambda x: x['age'])\nsorted_by_name = sorted(data, key=lambda x: x['name'])\n\n# With map, filter, reduce\nnumbers = [1, 2, 3, 4, 5]\n\n# Map: apply function to all elements\nsquared = list(map(lambda x: x**2, numbers))  # [1, 4, 9, 16, 25]\n\n# Filter: keep elements that satisfy condition\nevens = list(filter(lambda x: x % 2 == 0, numbers))  # [2, 4]\n\n# Reduce: aggregate to single value\nfrom functools import reduce\ntotal = reduce(lambda x, y: x + y, numbers)  # 15\n\n# In pandas (very common in interviews)\nimport pandas as pd\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n\n# Apply lambda to column\ndf['C'] = df['A'].apply(lambda x: x * 2 if x > 1 else 0)\n\n# Apply to entire dataframe\ndf['max'] = df.apply(lambda row: max(row['A'], row['B']), axis=1)\n\n# Complex sorting with multiple keys\nstudents = [\n    {'name': 'Alice', 'grade': 85, 'age': 20},\n    {'name': 'Bob', 'grade': 85, 'age': 19},\n    {'name': 'Charlie', 'grade': 90, 'age': 21}\n]\n# Sort by grade (descending), then age (ascending)\nsorted_students = sorted(students, key=lambda x: (-x['grade'], x['age']))\n\n# When NOT to use lambda\n# Too complex - use regular function:\n# BAD:\nresult = list(map(lambda x: x**2 if x > 0 else -x**2 if x < 0 else 0, numbers))\n\n# GOOD:\ndef transform(x):\n    if x > 0:\n        return x**2\n    elif x < 0:\n        return -x**2\n    else:\n        return 0\n\nresult = list(map(transform, numbers))\n```\n\n### Module 3 Exercises\n\n**Exercise 3.1 (Easy):** Write a function with default parameters that formats a number as currency.\n**Exercise 3.2 (Medium):** Use lambda with sorted() to sort list of tuples by second element.\n**Exercise 3.3 (Hard):** Create a function that returns a function (closure) for custom filtering.\n**Exercise 3.4 (Debug):** Fix the mutable default argument bug in the provided code.\n**Exercise 3.5 (Interview):** Use map, filter, and reduce to process a list of transactions.\n\n---\n\n## Module 4: Essential Pandas Operations\n\n### Day 8-11 Schedule\n- **Day 8:** DataFrame basics and selection (4.1)\n- **Day 9:** GroupBy operations (4.2)\n- **Day 10:** Merge operations (4.3)\n- **Day 11:** Missing values (4.4)\n- **Exercises:** Complete Module 4 exercises daily\n- **Flashcards:** \"Pattern Recognition\" category heavily\n\n### 4.1 DataFrame Basics\n\nDataFrames are the core of pandas - 2D labeled data structures with columns of potentially different types.\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# Creating DataFrames\n# From dictionary\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'age': [25, 30, 35],\n    'salary': [50000, 60000, 70000]\n})\n\n# From list of dictionaries\nrecords = [\n    {'name': 'Alice', 'age': 25},\n    {'name': 'Bob', 'age': 30}\n]\ndf = pd.DataFrame(records)\n\n# From CSV\ndf = pd.read_csv('data.csv')\n\n# Selection and filtering\n# Single column (returns Series)\nages = df['age']\n\n# Multiple columns (returns DataFrame)\nsubset = df[['name', 'age']]\n\n# Row selection by index\nfirst_row = df.iloc[0]  # By position\nrow_by_label = df.loc[0]  # By index label\n\n# Boolean indexing (CRUCIAL for interviews)\nadults = df[df['age'] >= 18]\nhigh_earners = df[df['salary'] > 60000]\ncomplex_filter = df[(df['age'] > 30) & (df['salary'] < 70000)]\n\n# CRITICAL: Avoid chained assignment\n# WRONG - raises SettingWithCopyWarning:\ndf[df['age'] > 30]['salary'] = 100000  # DON'T DO THIS\n\n# RIGHT - use .loc:\ndf.loc[df['age'] > 30, 'salary'] = 100000\n\n# Adding/modifying columns\ndf['bonus'] = df['salary'] * 0.1\ndf['full_name'] = df['first'] + ' ' + df['last']\ndf['category'] = np.where(df['salary'] > 60000, 'high', 'low')\n\n# Dropping columns\ndf_clean = df.drop(columns=['temp_col'])\n# Or in place\ndf.drop(columns=['temp_col'], inplace=True)\n\n# Reset index after operations\ndf_filtered = df[df['age'] > 25]\ndf_filtered = df_filtered.reset_index(drop=True)\n```\n\n### 4.2 GroupBy Operations\n\nGroupBy is the most important pandas pattern for analytics - splits data into groups and applies operations.\n\n```python\n# Basic groupby pattern\ndf = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'B', 'A'],\n    'value': [10, 20, 30, 40, 50],\n    'quantity': [1, 2, 3, 4, 5]\n})\n\n# Single column, single aggregation\ngrouped = df.groupby('category')['value'].sum()\n# Result: Series with category as index\n\n# Reset index to get DataFrame\ngrouped_df = df.groupby('category')['value'].sum().reset_index()\n\n# Multiple aggregations\nagg_result = df.groupby('category').agg({\n    'value': 'sum',\n    'quantity': 'mean'\n}).reset_index()\n\n# Multiple functions on same column\nmulti_agg = df.groupby('category')['value'].agg(['sum', 'mean', 'count'])\n\n# Custom aggregation functions\ndef peak_to_peak(x):\n    return x.max() - x.min()\n\ncustom_agg = df.groupby('category')['value'].agg(peak_to_peak)\n\n# Group by multiple columns\nmulti_group = df.groupby(['category', 'subcategory'])['value'].sum()\n\n# Transform: returns same-sized result\ndf['mean_by_category'] = df.groupby('category')['value'].transform('mean')\n\n# Filter: keep groups that meet condition\nlarge_groups = df.groupby('category').filter(lambda x: x['value'].sum() > 50)\n\n# Common interview pattern: percentage of total\ndf['pct_of_category'] = df.groupby('category')['value'].transform(lambda x: x / x.sum())\n```\n\n### 4.3 Merge Operations\n\nMerging/joining DataFrames is essential for combining data from multiple sources.\n\n```python\n# Sample DataFrames\ncustomers = pd.DataFrame({\n    'customer_id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David']\n})\n\norders = pd.DataFrame({\n    'order_id': [101, 102, 103],\n    'customer_id': [1, 2, 1],\n    'amount': [50, 75, 100]\n})\n\n# Inner join (default) - only matching keys\ninner = pd.merge(customers, orders, on='customer_id')\n\n# Left join - all from left, matching from right\nleft = pd.merge(customers, orders, on='customer_id', how='left')\n\n# Right join - all from right, matching from left\nright = pd.merge(customers, orders, on='customer_id', how='right')\n\n# Outer join - all from both\nouter = pd.merge(customers, orders, on='customer_id', how='outer')\n\n# Different column names\ndf1 = pd.DataFrame({'id': [1, 2], 'value': [10, 20]})\ndf2 = pd.DataFrame({'user_id': [1, 2], 'score': [100, 200]})\nmerged = pd.merge(df1, df2, left_on='id', right_on='user_id')\n\n# Multiple join keys\nmerged_multi = pd.merge(df1, df2, on=['key1', 'key2'])\n\n# Indicator flag (shows source of each row)\nmerged_indicated = pd.merge(df1, df2, on='key', how='outer', indicator=True)\n# _merge column shows 'left_only', 'right_only', or 'both'\n\n# Handling duplicates in merge\n# If duplicates in join key, creates cartesian product\n# Use validate parameter to check:\nmerged_validated = pd.merge(df1, df2, on='key', validate='one_to_one')\n# Options: 'one_to_one', 'one_to_many', 'many_to_one', 'many_to_many'\n```\n\n### 4.4 Missing Values\n\nHandling missing data is crucial for data quality and appears in every real-world dataset.\n\n```python\n# Creating DataFrame with missing values\ndf = pd.DataFrame({\n    'A': [1, 2, np.nan, 4],\n    'B': [5, np.nan, np.nan, 8],\n    'C': [9, 10, 11, 12]\n})\n\n# Detection\ndf.isnull()            # Boolean mask of missing values\ndf.isnull().sum()      # Count per column\ndf.isnull().sum().sum()  # Total missing\ndf.isnull().any()      # Which columns have any nulls\n\n# Dropping missing values\ndf.dropna()            # Drop rows with ANY missing\ndf.dropna(how='all')   # Drop rows with ALL missing\ndf.dropna(subset=['A', 'B'])  # Check specific columns\ndf.dropna(thresh=2)    # Keep rows with at least 2 non-null values\n\n# Filling missing values\ndf.fillna(0)           # Fill with constant\ndf.fillna(method='ffill')  # Forward fill (use previous value)\ndf.fillna(method='bfill')  # Backward fill (use next value)\n\n# Column-specific filling\ndf['A'].fillna(df['A'].mean(), inplace=True)  # Fill with mean\ndf['B'].fillna(df['B'].median(), inplace=True)  # Fill with median\ndf['C'].fillna(df['C'].mode()[0], inplace=True)  # Fill with mode\n\n# Interpolation\ndf.interpolate()       # Linear interpolation\ndf.interpolate(method='time')  # Time-based\n\n# Interview pattern: different strategies per column\nfill_values = {\n    'age': df['age'].mean(),\n    'salary': df['salary'].median(),\n    'category': 'unknown'\n}\ndf.fillna(value=fill_values, inplace=True)\n\n# Check for infinity values (often missed)\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\n```\n\n### Module 4 Exercises\n\n**Exercise 4.1 (Easy):** Create a DataFrame and calculate mean salary by department.\n**Exercise 4.2 (Medium):** Merge two DataFrames and handle missing values in the result.\n**Exercise 4.3 (Hard):** Group by multiple columns and calculate multiple aggregations.\n**Exercise 4.4 (Debug):** Fix the SettingWithCopyWarning in the provided code.\n**Exercise 4.5 (Interview):** Find top 3 products by revenue in each region, excluding nulls.\n\n---\n\n## Module 5: File I/O & Error Handling\n\n### Day 5 Schedule\n- **Morning:** Reading and writing files (5.1)\n- **Afternoon:** Error handling patterns (5.2)\n- **Exercises:** Complete Module 5 exercises\n- **Flashcards:** I/O and error patterns\n\n### 5.1 Reading and Writing Files\n\nFile I/O is essential for real-world data processing - you'll always be reading from and writing to files.\n\n```python\nimport pandas as pd\nimport json\nimport csv\n\n# CSV Files\n# Reading CSV\ndf = pd.read_csv('data.csv')\n\n# Common parameters\ndf = pd.read_csv(\n    'data.csv',\n    sep=',',           # Delimiter\n    header=0,          # Row with column names\n    index_col=0,       # Column to use as index\n    usecols=['col1', 'col2'],  # Specific columns\n    dtype={'col1': str},  # Specify dtypes\n    parse_dates=['date_col'],  # Parse as datetime\n    nrows=1000,        # Read only first n rows\n    skiprows=10,       # Skip first n rows\n    encoding='utf-8'   # Handle encoding\n)\n\n# Writing CSV\ndf.to_csv('output.csv', index=False)\n\n# JSON Files\n# Reading JSON\nwith open('data.json', 'r') as f:\n    data = json.load(f)\n\n# Writing JSON\nwith open('output.json', 'w') as f:\n    json.dump(data, f, indent=2)\n\n# Pandas JSON operations\ndf = pd.read_json('data.json')\ndf.to_json('output.json', orient='records')\n\n# Excel Files\ndf = pd.read_excel('data.xlsx', sheet_name='Sheet1')\ndf.to_excel('output.xlsx', sheet_name='Results', index=False)\n\n# Multiple sheets\nwith pd.ExcelWriter('output.xlsx') as writer:\n    df1.to_excel(writer, sheet_name='Sheet1')\n    df2.to_excel(writer, sheet_name='Sheet2')\n\n# Handle encoding errors (common in interviews)\ndef read_csv_safe(filepath):\n    encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n    \n    for encoding in encodings:\n        try:\n            return pd.read_csv(filepath, encoding=encoding)\n        except UnicodeDecodeError:\n            continue\n    \n    raise ValueError(f\"Could not read {filepath} with any encoding\")\n\n# Chunking large files (memory efficient)\nchunk_size = 10000\nchunks = []\n\nfor chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n    # Process each chunk\n    processed = chunk[chunk['value'] > 100]\n    chunks.append(processed)\n\nresult = pd.concat(chunks, ignore_index=True)\n```\n\n### 5.2 Error Handling\n\nRobust error handling separates production-ready code from scripts that break.\n\n```python\n# Basic try-except pattern\ntry:\n    result = risky_operation()\nexcept Exception as e:\n    print(f\"Operation failed: {e}\")\n    result = None\n\n# Multiple exception types\ntry:\n    df = pd.read_csv('file.csv')\n    value = df['column'][0]\n    calculation = 10 / value\nexcept FileNotFoundError:\n    print(\"File not found\")\n    df = pd.DataFrame()\nexcept KeyError as e:\n    print(f\"Column not found: {e}\")\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n\n# Finally clause (always runs)\ntry:\n    file = open('data.txt', 'r')\n    data = file.read()\nexcept IOError:\n    data = \"\"\nfinally:\n    file.close()  # Always close file\n\n# Better: use context manager\ntry:\n    with open('data.txt', 'r') as f:\n        data = f.read()\nexcept IOError:\n    data = \"\"\n\n# Retry logic pattern (common in data pipelines)\nimport time\n\ndef retry_operation(func, max_retries=3, delay=1):\n    \"\"\"\n    Retry a function with exponential backoff\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise  # Re-raise on final attempt\n            \n            wait_time = delay * (2 ** attempt)  # Exponential backoff\n            print(f\"Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...\")\n            time.sleep(wait_time)\n\n# Custom exceptions for clarity\nclass DataValidationError(Exception):\n    pass\n\ndef validate_data(df):\n    if df.empty:\n        raise DataValidationError(\"DataFrame is empty\")\n    if df.isnull().sum().sum() > len(df) * 0.5:\n        raise DataValidationError(\"Too many missing values\")\n    return True\n\n# Logging errors (professional approach)\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    df = pd.read_csv('data.csv')\n    logger.info(f\"Successfully loaded {len(df)} rows\")\nexcept Exception as e:\n    logger.error(f\"Failed to load data: {e}\")\n    raise\n```\n\n### Module 5 Exercises\n\n**Exercise 5.1 (Easy):** Write a function that safely reads a JSON file and returns empty dict on error.\n**Exercise 5.2 (Medium):** Read a CSV file and handle potential encoding errors.\n**Exercise 5.3 (Hard):** Implement retry logic with exponential backoff for API calls.\n**Exercise 5.4 (Debug):** Fix the file handling code that doesn't properly close files.\n**Exercise 5.5 (Interview):** Process large CSV in chunks and aggregate results.\n\n---\n\n## Module 6: Common Gotchas & Best Practices\n\n### Day 6 Schedule\n- **Morning:** Python gotchas that trip up interviews (6.1)\n- **Afternoon:** Performance optimization patterns (6.2)\n- **Exercises:** Complete Module 6 exercises\n- **Flashcards:** Heavy focus on \"Gotchas\" category\n\n### 6.1 Common Interview Gotchas\n\nThese are the \"implement without built-in functions\" problems that test your understanding of fundamentals.\n\n```python\n# GOTCHA 1: Sort without .sort() or sorted()\ndef bubble_sort(arr):\n    \"\"\"\n    Implement sorting manually - O(n\u00b2) complexity\n    \"\"\"\n    n = len(arr)\n    for i in range(n):\n        for j in range(i + 1, n):\n            if arr[i] > arr[j]:\n                arr[i], arr[j] = arr[j], arr[i]\n    return arr\n\n# GOTCHA 2: Group without .groupby()\ndef manual_groupby(data, key_func):\n    \"\"\"\n    Group data without pandas groupby\n    \"\"\"\n    groups = {}\n    for item in data:\n        key = key_func(item)\n        if key not in groups:\n            groups[key] = []\n        groups[key].append(item)\n    return groups\n\n# Usage\nrecords = [\n    {'category': 'A', 'value': 10},\n    {'category': 'B', 'value': 20},\n    {'category': 'A', 'value': 30}\n]\ngrouped = manual_groupby(records, lambda x: x['category'])\n\n# GOTCHA 3: Remove duplicates without .drop_duplicates() or set()\ndef remove_duplicates_manual(items):\n    \"\"\"\n    Remove duplicates while preserving order\n    \"\"\"\n    seen = []\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.append(item)\n            result.append(item)\n    return result\n\n# GOTCHA 4: Find max without max()\ndef find_max(numbers):\n    \"\"\"\n    Find maximum value manually\n    \"\"\"\n    if not numbers:\n        return None\n    \n    max_val = numbers[0]\n    for num in numbers[1:]:\n        if num > max_val:\n            max_val = num\n    return max_val\n\n# GOTCHA 5: Flatten nested list without itertools\ndef flatten(nested):\n    \"\"\"\n    Flatten arbitrarily nested list\n    \"\"\"\n    result = []\n    for item in nested:\n        if isinstance(item, list):\n            result.extend(flatten(item))\n        else:\n            result.append(item)\n    return result\n\n# GOTCHA 6: Integer overflow (Python handles automatically, but know this)\n# In Python 3, integers have unlimited precision\nbig_num = 10 ** 100  # No overflow\n\n# GOTCHA 7: Floating point precision\n0.1 + 0.2 == 0.3  # False! Due to floating point representation\n# Use approximate comparison:\nabs((0.1 + 0.2) - 0.3) < 1e-10  # True\n\n# GOTCHA 8: List modification during iteration\nnumbers = [1, 2, 3, 4, 5]\n# WRONG:\nfor i, num in enumerate(numbers):\n    if num % 2 == 0:\n        del numbers[i]  # Modifies list being iterated\n\n# RIGHT:\nnumbers = [num for num in numbers if num % 2 != 0]\n# Or iterate over copy:\nfor num in numbers[:]:\n    if num % 2 == 0:\n        numbers.remove(num)\n```\n\n### 6.2 Performance Optimization\n\nUnderstanding performance helps you discuss scalability in interviews.\n\n```python\n# Use sets for membership testing\n# SLOW - O(n) for each lookup:\nlarge_list = list(range(1000000))\nif 500000 in large_list:  # Takes time\n    pass\n\n# FAST - O(1) average case:\nlarge_set = set(range(1000000))\nif 500000 in large_set:  # Instant\n    pass\n\n# Use vectorized pandas operations\n# SLOW - iterating over DataFrame:\nfor i in range(len(df)):\n    df.loc[i, 'new'] = df.loc[i, 'old'] * 2\n\n# FAST - vectorized operation:\ndf['new'] = df['old'] * 2\n\n# Use .itertuples() instead of .iterrows()\n# SLOW:\nfor index, row in df.iterrows():\n    process(row['column'])\n\n# FASTER:\nfor row in df.itertuples():\n    process(row.column)\n\n# String concatenation\n# SLOW - creates new string each time:\nresult = \"\"\nfor word in words:\n    result += word + \" \"\n\n# FAST - join at once:\nresult = \" \".join(words)\n\n# List comprehension vs loops\n# SLOW:\nsquares = []\nfor x in range(1000):\n    squares.append(x**2)\n\n# FAST - list comprehension:\nsquares = [x**2 for x in range(1000)]\n\n# FASTER - generator for large data:\nsquares = (x**2 for x in range(1000))\n\n# Dictionary get() vs checking keys\n# SLOW:\nif key in dict:\n    value = dict[key]\nelse:\n    value = default\n\n# FAST:\nvalue = dict.get(key, default)\n\n# Preallocate lists when size is known\n# SLOW:\nresult = []\nfor i in range(n):\n    result.append(process(i))\n\n# FASTER:\nresult = [None] * n\nfor i in range(n):\n    result[i] = process(i)\n\n# Use Counter for counting\nfrom collections import Counter\n\n# Manual counting - slower:\ncounts = {}\nfor item in items:\n    counts[item] = counts.get(item, 0) + 1\n\n# Counter - faster and cleaner:\ncounts = Counter(items)\n```\n\n### Module 6 Exercises\n\n**Exercise 6.1 (Easy):** Implement bubble sort without using .sort() or sorted().\n**Exercise 6.2 (Medium):** Group a list of dictionaries by a key without using groupby.\n**Exercise 6.3 (Hard):** Flatten an arbitrarily nested list structure.\n**Exercise 6.4 (Debug):** Fix the code that modifies a list while iterating.\n**Exercise 6.5 (Interview):** Optimize slow pandas code to use vectorized operations.\n\n---\n\n# PART 2: PATTERN INTEGRATION\n\n## Week 2: Connecting Concepts to Interview Patterns\n\n### Days 8-14: Pandas Patterns in Practice\n\nNow that you understand Python fundamentals, we'll apply them to the patterns you'll see in interviews.\n\n**Daily Pattern Practice Schedule:**\n- **Day 8:** Basic operations become filtering patterns\n- **Day 9:** GroupBy becomes aggregation patterns\n- **Day 10:** Merge becomes join patterns\n- **Day 11:** Missing data becomes cleaning patterns\n- **Day 12:** Advanced operations become complex patterns\n- **Day 13:** Time series patterns\n- **Day 14:** Integration day - combine multiple patterns\n\nEach pattern maps directly to Module 4 concepts, but now we practice them as interview questions:\n\n**From concept to pattern:**\n- \"Group and sum\" \u2192 \"Find total revenue by category\"\n- \"Filter and select\" \u2192 \"Get top customers from last month\"\n- \"Merge and clean\" \u2192 \"Combine sales and customer data, handle mismatches\"\n\n---\n\n## Week 3: Interview Simulation\n\n### Days 15-21: Speed and Polish\n\n**Pattern Speed Benchmarks:**\n- Simple aggregation: < 2 minutes\n- Top-N pattern: < 3 minutes\n- Complex join with cleaning: < 5 minutes\n- Multi-step analysis: < 10 minutes\n\n**Daily Focus:**\n- **Day 15:** Pattern recognition speed\n- **Day 16:** Gotcha problems\n- **Day 17:** Mock interview #1\n- **Day 18:** Talking points and explanations\n- **Day 19:** Mock interview #2\n- **Day 20:** Company-specific preparation\n- **Day 21:** Final review and rest\n\n---\n\n## Study Strategies\n\n### For Different Timelines\n\n**1 Week Timeline:**\n- Skip to Module 4 (Pandas)\n- Focus on GroupBy, Merge, Missing Data\n- Learn Module 6 gotchas\n- Practice 5 patterns daily\n\n**2 Week Timeline:**\n- Week 1: Modules 1-3 (accelerated, 2 modules/day)\n- Week 2: Modules 4-6 with heavy practice\n\n**3 Week Timeline:**\n- Follow the full 21-day schedule\n- Add extra mock interviews\n- Build portfolio project\n\n### Daily Routine\n\n**Morning (30 min):**\n1. Review yesterday's flashcards\n2. Read new module section\n3. Run example code\n\n**Lunch (15 min):**\n- Quick flashcard review\n- One speed drill\n\n**Evening (45-60 min):**\n1. Complete exercises\n2. Practice patterns from memory\n3. Update flashcards with mistakes\n\n### Flashcard Integration\n\n**Module \u2192 Flashcard Mapping:**\n- Module 1 \u2192 Data structure operations cards\n- Module 2 \u2192 Comprehension pattern cards\n- Module 3 \u2192 Lambda and function cards\n- Module 4 \u2192 All Pattern Recognition cards\n- Module 5 \u2192 I/O and error handling cards\n- Module 6 \u2192 All Gotcha cards\n\n**Cram Mode Schedule:**\n- Days 1-7: Add new cards daily\n- Days 8-14: Full deck review\n- Days 15-21: Focus on problem cards\n\n---\n\n## Assessment Checkpoints\n\n### After Module 1-2 (Day 3):\n- [ ] Can create and manipulate lists, dicts, sets\n- [ ] Can write list comprehensions\n- [ ] Understand string operations\n\n### After Module 3-4 (Day 10):\n- [ ] Can write functions with proper defaults\n- [ ] Comfortable with lambda functions\n- [ ] Can perform all basic pandas operations\n\n### After Module 5-6 (Day 14):\n- [ ] Can handle files and errors gracefully\n- [ ] Can implement gotchas from scratch\n- [ ] Understand performance implications\n\n### Final Check (Day 21):\n- [ ] Can solve 3 problems in 45 minutes\n- [ ] Can explain approach while coding\n- [ ] Can discuss tradeoffs and complexity\n- [ ] Flashcards at 95% accuracy\n\n---\n\n## Quick Reference Sheet\n\n### Must-Know Patterns\n```python\n# Top-N by group\ndf.sort_values('value', ascending=False).groupby('group').head(n)\n\n# GroupBy aggregate\ndf.groupby('category').agg({'value': 'sum', 'count': 'mean'})\n\n# Merge with indicator\npd.merge(df1, df2, on='key', how='left', indicator=True)\n\n# Handle missing\ndf.fillna({'col1': 0, 'col2': 'unknown', 'col3': df['col3'].mean()})\n\n# Remove duplicates preserving order\ndf.drop_duplicates(subset=['col1', 'col2'], keep='first')\n```\n\n### Interview Mantras\n1. \"Let me clarify the requirements first...\"\n2. \"I'll start with a simple approach, then optimize...\"\n3. \"The time complexity of this approach is...\"\n4. \"An alternative approach would be...\"\n5. \"Let me test this with an edge case...\"\n\n---\n\n## Troubleshooting Guide\n\n**\"I keep forgetting syntax\"**\n- Drill flashcards 2x daily\n- Write patterns from memory daily\n- Use quick_reference.md\n\n**\"I'm too slow\"**\n- Time every practice problem\n- Use keyboard shortcuts\n- Practice typing common patterns\n\n**\"I panic in mock interviews\"**\n- Practice explaining aloud daily\n- Record yourself solving problems\n- Do more mock interviews\n\n**\"Concepts don't connect\"**\n- Review module \u2192 pattern mapping\n- Draw connections on paper\n- Explain to someone else\n\n---\n\n*Remember: The goal is not perfection but proficiency. Focus on being good enough to pass the screen and demonstrate you can learn on the job.*",
  "docs/talking_points.md": "# Talking Points - How to Discuss Your Solutions Professionally\n\n## The Meta-Strategy\n\n**Remember:** They're not just evaluating your code. They're evaluating whether they want to work with you for the next 2+ years.\n\nYour communication style matters as much as your solution. You need to sound like someone who:\n- Thinks systematically\n- Considers tradeoffs\n- Collaborates well\n- Can explain technical concepts to non-technical stakeholders\n\n## Part 1: Starting the Problem\n\n### The Opening Framework\n\n**What they say:** \"Find the top 3 products by revenue in each region\"\n\n**What NOT to do:** \n- Immediately start coding\n- Say \"that's easy\" (even if it is)\n- Make assumptions without confirming\n\n**What TO do:**\n```\n\"Let me make sure I understand the requirements correctly:\n- We want the top 3 products ranked by revenue\n- We need this breakdown for each region separately\n- Should I assume the data is already clean, or should I handle nulls?\n- Any specific tie-breaking logic if products have equal revenue?\n- Is there a minimum threshold for inclusion?\"\n```\n\n### Clarifying Questions to Always Ask\n\n**For Data Problems:**\n- \"What's the approximate size of the dataset?\"\n- \"Should I handle missing or invalid data?\"\n- \"Are there any edge cases I should consider?\"\n- \"What should happen with ties?\"\n- \"Any performance constraints I should know about?\"\n\n**For General Problems:**\n- \"Can you provide an example input and expected output?\"\n- \"What should happen with empty inputs?\"\n- \"Should I optimize for time or space?\"\n- \"Will this run once or repeatedly?\"\n\n## Part 2: Explaining Your Approach\n\n### The Three-Step Pattern\n\n**Step 1: High-level approach**\n```\n\"I'll approach this in three steps:\n1. First, clean the data and handle any nulls\n2. Then, group by region and calculate revenue\n3. Finally, rank within each group and select top 3\"\n```\n\n**Step 2: Start simple**\n```\n\"Let me start with a straightforward solution to ensure correctness,\nthen we can optimize if needed.\"\n```\n\n**Step 3: Think aloud (selectively)**\n```\n\"I'm using sort_values before groupby here because...\nActually, let me use nlargest instead, it's more efficient for this case.\"\n```\n\n### Key Phrases That Show Expertise\n\n**Good phrases to use:**\n- \"The tradeoff here is...\"\n- \"In production, I would also consider...\"\n- \"This assumes that...\"\n- \"An alternative approach would be...\"\n- \"Let me trace through an example...\"\n- \"The edge case here would be...\"\n\n**Avoid these phrases:**\n- \"I think this might work...\"\n- \"I'm not sure but...\"\n- \"Is this right?\"\n- \"I forgot how to...\"\n- \"This is probably wrong but...\"\n\n## Part 3: Complexity Analysis\n\n### How to Discuss Big O Naturally\n\n**DON'T memorize and recite:**\n\"This algorithm has O(n log n) time complexity and O(n) space complexity.\"\n\n**DO explain in context:**\n```\n\"The sorting step is O(n log n), which dominates the overall complexity.\nThe groupby is O(n), so total time is O(n log n).\nWe're storing all the data once, so space is O(n).\n\nFor our use case with ~10k products, this is perfectly fine.\nIf we had millions of products, we might want to consider...\"\n```\n\n### Common Complexity Discussions\n\n**For Pandas Operations:**\n```\n\"GroupBy is generally O(n) as it scans once through the data.\nThe sort within each group adds O(k log k) where k is group size.\nSince groups are small relative to total data, this is efficient.\"\n```\n\n**For Manual Implementations:**\n```\n\"This nested loop gives us O(n\u00b2), which is fine for small datasets.\nFor larger data, I'd use a hash map to get O(n) instead.\"\n```\n\n**For Space Complexity:**\n```\n\"We're creating a copy of the filtered data, so space is O(m) \nwhere m is the number of matching records.\nWe could reduce this by using views instead of copies.\"\n```\n\n## Part 4: Handling Feedback\n\n### When They Say \"Can you optimize this?\"\n\n**Good response:**\n```\n\"Sure! The current solution is O(n\u00b2). \nThe bottleneck is this nested loop.\nWe can optimize by using a dictionary for O(1) lookups,\nbringing total complexity down to O(n).\nWould you like me to implement that?\"\n```\n\n**Not good:**\n\"Um, I guess I could try to make it faster somehow...\"\n\n### When They Say \"What if the data was 100x larger?\"\n\n**Good response:**\n```\n\"At that scale, we'd need to consider:\n1. Memory constraints - might not fit in RAM\n2. Processing in chunks using chunking or Dask\n3. Perhaps pushing this logic to the database layer\n4. Using columnar storage formats like Parquet\n\nFor this interview, should I implement the chunking approach?\"\n```\n\n### When They Say \"Is there another way?\"\n\n**Good response:**\n```\n\"Yes, actually there are a few alternatives:\n1. We could use a heap for better memory efficiency\n2. We could use SQL if this data is in a database\n3. We could use rank() instead of sort and filter\n\nWhich approach would you prefer to see?\"\n```\n\n### When They Point Out a Bug\n\n**Good response:**\n```\n\"You're absolutely right, I see the issue.\nWhen the list is empty, this would raise an IndexError.\nLet me add a check for that case.\n[Fix it immediately]\nGood catch - in production, I'd also add a test for this edge case.\"\n```\n\n**Not good:**\n\"Oh no, where? I don't see it... wait... oh maybe here?\"\n\n## Part 5: Domain-Specific Value Adds\n\n### Connecting to Business Value\n\nWork these in naturally when relevant:\n\n**Data Quality:**\n```\n\"In my experience, revenue data often has quality issues,\nso I'm adding validation to catch negative values or outliers\nthat might indicate data problems upstream.\"\n```\n\n**Scale Considerations:**\n```\n\"This solution works well for daily reporting.\nIf this were for real-time dashboards, I'd consider\npre-aggregating or using a materialized view.\"\n```\n\n**Governance:**\n```\n\"If this touched PII, we'd need to consider data governance.\nI'd typically implement this in the semantic layer to ensure\nconsistent business logic across all consumers.\"\n```\n\n### Mentioning Your Advanced Skills (Subtly)\n\n**When appropriate:**\n```\n\"This reminds me of a similar problem I solved using dbt models,\nwhere we needed consistent metric definitions across teams.\"\n\n\"In production, I'd expose this through a semantic layer\nso other tools could access the same business logic.\"\n\n\"We could even make this available to AI tools through MCP,\nmaintaining governance while enabling self-service.\"\n```\n\n**But DON'T force it:**\n- Only mention if genuinely relevant\n- Keep it brief\n- Return focus to the problem at hand\n\n## Part 6: Professional Patterns\n\n### The \"Teaching\" Pattern\n\nWhen explaining, act like you're helping a junior understand:\n```\n\"Let me break down what this groupby is doing:\nFirst, it segments our data by region...\nThen, within each segment, we calculate revenue...\nFinally, we rank and select top 3...\"\n```\n\n### The \"Collaboration\" Pattern\n\nMake it feel like teamwork:\n```\n\"What do you think about this approach?\"\n\"Would you prefer to see the recursive or iterative version?\"\n\"Should we prioritize readability or performance here?\"\n```\n\n### The \"Production Mindset\" Pattern\n\nShow you think beyond the interview:\n```\n\"For this exercise, I'll keep it simple, but in production I'd add:\n- Input validation\n- Error handling  \n- Logging\n- Tests\"\n```\n\n### The \"Debugging\" Pattern\n\nWhen something doesn't work:\n```\n\"Let me trace through this with a simple example...\nIf input is [1, 2, 3], then...\nAh, I see the issue - the index is off by one.\nLet me fix that.\"\n```\n\n## Part 7: Handling Pressure\n\n### When You Blank Out\n\n**Say:**\n```\n\"Let me think through this step by step.\nActually, can I have 30 seconds to organize my thoughts?\"\n[Take a breath, think, then continue]\n```\n\n### When You Don't Know Something\n\n**Good:**\n```\n\"I haven't used that specific method before,\nbut based on the name, I'd expect it to...\nIn my current role, I typically solve this using...\nCould you give me a hint about that method?\"\n```\n\n**Bad:**\n\"I don't know.\"\n[Silence]\n\n### When Time is Running Out\n\n**Say:**\n```\n\"I see we're running short on time.\nLet me quickly outline how I'd finish this:\n1. Add error handling here\n2. Optimize this loop  \n3. Add tests for edge cases\nThe key insight is that we're trading space for time...\"\n```\n\n## Part 8: Closing Strong\n\n### Summarizing Your Solution\n\n**End with:**\n```\n\"To summarize, this solution:\n- Handles the requirements by doing X, Y, Z\n- Runs in O(n log n) time, which is acceptable for the data size\n- Includes error handling for edge cases\n- Could be extended to handle [additional requirement] if needed\n\nAny questions about my approach?\"\n```\n\n### Asking Good Questions\n\n**Show interest:**\n```\n\"How does your team currently solve this type of problem?\"\n\"What scale does this actually run at in your system?\"\n\"Are there other constraints I didn't consider?\"\n```\n\n## Quick Reference: Power Phrases\n\n### Starting\n- \"Let me understand the requirements...\"\n- \"Before I code, let me clarify...\"\n- \"I'll start with a simple approach...\"\n\n### During\n- \"The tradeoff here is...\"\n- \"Let me trace through an example...\"\n- \"A more efficient approach would be...\"\n\n### Debugging\n- \"Let me check my logic here...\"\n- \"I see the issue...\"\n- \"Good catch, let me fix that...\"\n\n### Ending\n- \"To summarize...\"\n- \"The key insight is...\"\n- \"In production, I would also...\"\n\n## The Mindset\n\nRemember: You're not begging for a job. You're having a technical discussion with a potential colleague. \n\nBe confident but not arrogant.\nBe thorough but not slow.\nBe smart but not condescending.\n\nThey should leave thinking: \"I want this person on my team.\"\n\n---\n\n## Emergency Phrases\n\nWhen completely stuck, these can buy you time:\n\n1. \"Let me make sure I understand what's being asked here...\"\n2. \"I want to think about the edge cases for a moment...\"\n3. \"Let me trace through this with a concrete example...\"\n4. \"I'm considering two approaches, let me think about tradeoffs...\"\n5. \"This reminds me of a similar problem, but let me adapt it...\"\n\n---\n\n## Final Note\n\nThe goal isn't to sound perfect. It's to sound like someone who:\n- Thinks before acting\n- Considers multiple solutions\n- Communicates clearly\n- Would be pleasant to work with\n- Can handle production systems\n\nYour MCP/dbt expertise will naturally come through in how you think about data problems. Don't force it, but don't hide it either.\n\nGood luck. You've got this.",
  "docs/quick_reference.md": "# Quick Reference - Python Analytics Engineering Syntax\n\nRapid lookup during interviews. No explanations, just syntax.\n\n## Lists\n\n```python\nlst = [1, 2, 3]\nlst.append(4)                 # Add single item\nlst.extend([5, 6])            # Add multiple items\nlst.insert(0, 0)              # Insert at index\nlst.remove(3)                 # Remove first occurrence\nlst.pop()                     # Remove & return last\nlst.pop(0)                    # Remove & return at index\nlst.clear()                   # Remove all\nlst.index(2)                  # Find index of value\nlst.count(2)                  # Count occurrences\nlst.sort()                    # Sort in place\nlst.reverse()                 # Reverse in place\nsorted(lst)                   # Return sorted copy\nreversed(lst)                 # Return iterator\nlst[:]                        # Shallow copy\nlst[1:3]                      # Slice [start:stop]\nlst[::2]                      # Slice with step\nlst[::-1]                     # Reverse slice\n```\n\n## Dictionaries\n\n```python\nd = {'a': 1, 'b': 2}\nd['c'] = 3                    # Add/update\nd.get('a')                    # Get with None default\nd.get('x', 0)                 # Get with custom default\nd.pop('a')                    # Remove & return\nd.popitem()                   # Remove & return arbitrary\nd.clear()                     # Remove all\nd.keys()                      # View of keys\nd.values()                    # View of values\nd.items()                     # View of (key, value)\nd.update({'c': 3})           # Update from dict\nd.setdefault('d', 4)         # Set if missing\n'a' in d                      # Check key exists\ndict.fromkeys(['a','b'], 0)  # Create with default value\n```\n\n## Sets\n\n```python\ns = {1, 2, 3}\ns.add(4)                      # Add element\ns.update([5, 6])              # Add multiple\ns.remove(2)                   # Remove (error if missing)\ns.discard(2)                  # Remove (no error)\ns.pop()                       # Remove & return arbitrary\ns.clear()                     # Remove all\ns1 | s2                       # Union\ns1 & s2                       # Intersection\ns1 - s2                       # Difference\ns1 ^ s2                       # Symmetric difference\ns1.issubset(s2)              # s1 <= s2\ns1.issuperset(s2)            # s1 >= s2\n```\n\n## Strings\n\n```python\ns = \"  Hello World  \"\ns.strip()                     # Remove whitespace\ns.lstrip()                    # Remove left whitespace\ns.rstrip()                    # Remove right whitespace\ns.lower()                     # Lowercase\ns.upper()                     # Uppercase\ns.capitalize()                # First letter uppercase\ns.title()                     # Title Case\ns.replace('o', 'a')          # Replace all\ns.split()                     # Split on whitespace\ns.split(',')                  # Split on delimiter\n','.join(['a','b'])          # Join with delimiter\ns.startswith('He')           # Check start\ns.endswith('ld')             # Check end\ns.find('o')                   # Find index (-1 if not found)\ns.count('l')                  # Count occurrences\ns.isdigit()                   # Check if all digits\ns.isalpha()                   # Check if all letters\ns.isalnum()                   # Check if alphanumeric\nf\"{var}\"                      # f-string\n\"{} {}\".format(a, b)         # Format method\n```\n\n## List Comprehensions\n\n```python\n[x for x in lst]              # Basic\n[x for x in lst if x > 0]    # With filter\n[x*2 for x in lst]            # With transformation\n[x if x > 0 else 0 for x in lst]  # Conditional expression\n[(x, y) for x in lst1 for y in lst2]  # Nested loops\n[item for sublist in lst for item in sublist]  # Flatten\n```\n\n## Dictionary Comprehensions\n\n```python\n{k: v for k, v in items}      # Basic\n{k: v for k, v in items if v > 0}  # With filter\n{k: v*2 for k, v in items}    # Transform values\n{v: k for k, v in items}      # Swap keys/values\n```\n\n## Lambda Functions\n\n```python\nlambda x: x * 2               # Basic\nlambda x, y: x + y            # Multiple args\nlambda x: x if x > 0 else 0   # Conditional\nsorted(lst, key=lambda x: x[1])  # Sort by element\nmap(lambda x: x*2, lst)       # Apply to all\nfilter(lambda x: x > 0, lst)  # Filter elements\n```\n\n## Functions\n\n```python\ndef func(a, b=1, *args, **kwargs):\n    return result\n\nfunc(1)                       # Positional\nfunc(a=1, b=2)               # Keyword\nfunc(1, 2, 3, 4)             # Extra positional \u2192 args\nfunc(1, x=5, y=6)            # Extra keyword \u2192 kwargs\n*lst                          # Unpack list to args\n**dict                        # Unpack dict to kwargs\n```\n\n## File I/O\n\n```python\n# Text files\nwith open('file.txt', 'r') as f:\n    content = f.read()        # Read all\n    lines = f.readlines()     # Read lines to list\n    line = f.readline()       # Read one line\n\nwith open('file.txt', 'w') as f:\n    f.write('text')           # Write string\n    f.writelines(lst)         # Write list of strings\n\n# JSON\nimport json\ndata = json.load(f)           # Read from file\njson.dump(data, f)            # Write to file\ns = json.dumps(data)          # To string\ndata = json.loads(s)          # From string\n```\n\n## Error Handling\n\n```python\ntry:\n    risky_operation()\nexcept ValueError as e:\n    handle_value_error(e)\nexcept (KeyError, IndexError):\n    handle_key_or_index()\nexcept Exception as e:\n    handle_any_error(e)\nelse:\n    runs_if_no_exception()\nfinally:\n    always_runs()\n```\n\n## Pandas - DataFrames\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# Creation\ndf = pd.DataFrame(data)\ndf = pd.read_csv('file.csv')\ndf = pd.read_excel('file.xlsx')\ndf = pd.read_json('file.json')\n\n# Info\ndf.shape                      # (rows, cols)\ndf.dtypes                     # Column types\ndf.info()                     # Overview\ndf.describe()                 # Statistics\ndf.head(n)                    # First n rows\ndf.tail(n)                    # Last n rows\ndf.sample(n)                  # Random n rows\n\n# Selection\ndf['col']                     # Select column\ndf[['col1', 'col2']]         # Select multiple columns\ndf.loc[row, col]             # Select by label\ndf.iloc[row, col]            # Select by position\ndf.loc[df['col'] > 5]        # Boolean indexing\n\n# Modification\ndf['new'] = values           # Add column\ndf.drop(columns=['col'])     # Drop column\ndf.drop(index=[0,1])         # Drop rows\ndf.rename(columns={'old':'new'})  # Rename\ndf.sort_values('col')        # Sort by column\ndf.sort_values(['col1','col2'])  # Sort by multiple\ndf.reset_index(drop=True)    # Reset index\n```\n\n## Pandas - Operations\n\n```python\n# Missing Data\ndf.isnull()                   # Boolean mask\ndf.notnull()                  # Inverse mask\ndf.isnull().sum()            # Count nulls per column\ndf.dropna()                   # Drop rows with nulls\ndf.dropna(axis=1)            # Drop columns with nulls\ndf.fillna(0)                  # Fill with value\ndf.fillna(method='ffill')    # Forward fill\ndf.fillna(method='bfill')    # Backward fill\ndf.interpolate()              # Interpolate\n\n# Duplicates\ndf.duplicated()               # Boolean mask\ndf.drop_duplicates()          # Remove duplicates\ndf.drop_duplicates(subset=['col'])  # By columns\ndf.drop_duplicates(keep='last')     # Keep last\n\n# Apply Functions\ndf['col'].apply(func)         # Apply to column\ndf.apply(func, axis=1)        # Apply to rows\ndf.applymap(func)            # Apply to all elements\ndf['col'].map(dict)          # Map values\ndf['col'].replace({old: new}) # Replace values\n```\n\n## Pandas - GroupBy\n\n```python\n# Basic GroupBy\ndf.groupby('col')['val'].sum()\ndf.groupby('col')['val'].mean()\ndf.groupby('col')['val'].count()\ndf.groupby('col')['val'].min()\ndf.groupby('col')['val'].max()\n\n# Multiple Aggregations\ndf.groupby('col').agg({'val1': 'sum', 'val2': 'mean'})\ndf.groupby('col')['val'].agg(['sum', 'mean', 'count'])\n\n# Named Aggregations\ndf.groupby('col').agg(\n    total=('val', 'sum'),\n    average=('val', 'mean')\n)\n\n# Transform (keep original shape)\ndf.groupby('col')['val'].transform('mean')\n\n# Filter groups\ndf.groupby('col').filter(lambda x: x['val'].sum() > 100)\n```\n\n## Pandas - Merge/Join\n\n```python\n# Merge\npd.merge(df1, df2, on='key')\npd.merge(df1, df2, on=['key1', 'key2'])\npd.merge(df1, df2, left_on='lkey', right_on='rkey')\npd.merge(df1, df2, how='left')   # left, right, inner, outer\npd.merge(df1, df2, indicator=True)  # Add _merge column\n\n# Concat\npd.concat([df1, df2])         # Stack vertically\npd.concat([df1, df2], axis=1) # Side by side\npd.concat([df1, df2], ignore_index=True)\n\n# Join (on index)\ndf1.join(df2)\n```\n\n## Pandas - Time Series\n\n```python\n# Parse Dates\npd.to_datetime(df['col'])\npd.to_datetime(df['col'], format='%Y-%m-%d')\n\n# Date Components\ndf['date'].dt.year\ndf['date'].dt.month\ndf['date'].dt.day\ndf['date'].dt.dayofweek\ndf['date'].dt.day_name()\ndf['date'].dt.quarter\n\n# Date Arithmetic\ndf['date'] + pd.Timedelta(days=1)\ndf['date'] - pd.Timedelta(hours=2)\n(df['date1'] - df['date2']).dt.days\n\n# Resampling\ndf.set_index('date').resample('D').sum()  # Daily\ndf.set_index('date').resample('M').mean() # Monthly\n\n# Rolling Windows\ndf['col'].rolling(window=7).mean()\ndf['col'].rolling(window=7).sum()\ndf['col'].expanding().sum()   # Cumulative\n```\n\n## Pandas - Advanced\n\n```python\n# Pivot Tables\npd.pivot_table(df, values='val', index='row', columns='col')\npd.pivot_table(df, values='val', index='row', aggfunc='sum')\n\n# Rank\ndf['rank'] = df['val'].rank()\ndf['rank'] = df['val'].rank(method='dense')\ndf['rank'] = df.groupby('group')['val'].rank()\n\n# Shift/Lag\ndf['prev'] = df['val'].shift(1)\ndf['next'] = df['val'].shift(-1)\ndf['pct_change'] = df['val'].pct_change()\n\n# Cumulative\ndf['cumsum'] = df['val'].cumsum()\ndf['cumsum'] = df.groupby('group')['val'].cumsum()\n\n# Cut/Bins\npd.cut(df['val'], bins=3)\npd.cut(df['val'], bins=[0, 10, 20, 30])\npd.qcut(df['val'], q=4)      # Quartiles\n```\n\n## NumPy Essentials\n\n```python\n# Arrays\narr = np.array([1, 2, 3])\narr = np.zeros(5)\narr = np.ones((3, 3))\narr = np.arange(0, 10, 2)\narr = np.linspace(0, 10, 5)\narr = np.random.randn(10)\n\n# Operations\nnp.sum(arr)\nnp.mean(arr)\nnp.std(arr)\nnp.min(arr)\nnp.max(arr)\nnp.argmin(arr)                # Index of min\nnp.argmax(arr)                # Index of max\n\n# Conditions\nnp.where(arr > 0, arr, 0)     # If-else\nnp.select([cond1, cond2], [val1, val2], default)\n```\n\n## Common Patterns - Quick Copy\n\n```python\n# Top N by group\ndf.sort_values('val', ascending=False).groupby('group').head(n)\ndf.groupby('group').apply(lambda x: x.nlargest(n, 'val'))\n\n# Percentage of total\ndf['pct'] = df['val'] / df['val'].sum() * 100\ndf['pct'] = df.groupby('group')['val'].transform(lambda x: x / x.sum())\n\n# Remove outliers (3 std)\nmean = df['val'].mean()\nstd = df['val'].std()\ndf = df[(df['val'] > mean - 3*std) & (df['val'] < mean + 3*std)]\n\n# Memory optimization\nfor col in df.select_dtypes(['object']):\n    df[col] = df[col].astype('category')\n```\n\n## Performance Tips\n\n```python\n# SLOW \u2192 FAST\n# Iterrows \u2192 Vectorization\nfor i, row in df.iterrows():  # AVOID\ndf['new'] = df['old'] * 2     # PREFER\n\n# Apply \u2192 Vectorization\ndf.apply(lambda x: x*2)        # SLOW\ndf * 2                         # FAST\n\n# Python loop \u2192 NumPy\n[x*2 for x in lst]            # SLOW\nnp.array(lst) * 2             # FAST\n\n# Append in loop \u2192 Concat once\nfor data in chunks:\n    df = df.append(data)       # SLOW\ndfs = [process(chunk) for chunk in chunks]\ndf = pd.concat(dfs)           # FAST\n```\n\n## Method Chains\n\n```python\n# Clean pattern\nresult = (df\n    .dropna()\n    .groupby('category')\n    .agg({'value': 'sum'})\n    .sort_values('value', ascending=False)\n    .head(10)\n    .reset_index()\n)\n```\n\n## Gotchas to Remember\n\n```python\n# Mutable defaults\ndef f(lst=[]):  # WRONG - shared across calls\ndef f(lst=None):  # RIGHT\n    if lst is None: lst = []\n\n# Modify while iterate\nfor item in lst:\n    if condition:\n        lst.remove(item)  # WRONG - skips elements\n\n# Use instead:\nlst = [x for x in lst if not condition]  # RIGHT\n\n# SettingWithCopyWarning\ndf[df['col'] > 0]['col2'] = 1  # WRONG\ndf.loc[df['col'] > 0, 'col2'] = 1  # RIGHT\n\n# Integer division\n3 / 2    # 1.5 in Python 3\n3 // 2   # 1 (floor division)\n```\n\n---\n\n*Copy what you need. No time for explanations during interviews.*",
  "docker/Dockerfile": "# Python Analytics Interview Prep - Complete Development Environment\n# Full Ubuntu Desktop with Wing Pro 11 and Python 3.13.6\n\nFROM ubuntu:22.04\n\n# Prevent interactive prompts during build\nENV DEBIAN_FRONTEND=noninteractive\nENV TZ=America/New_York\n\n# Install desktop environment and VNC\nRUN apt-get update && apt-get install -y \\\n    xfce4 \\\n    xfce4-terminal \\\n    xfce4-goodies \\\n    tightvncserver \\\n    novnc \\\n    websockify \\\n    supervisor \\\n    sudo \\\n    wget \\\n    curl \\\n    git \\\n    nano \\\n    software-properties-common \\\n    dbus-x11 \\\n    x11-utils \\\n    x11-xserver-utils \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python 3.13.6 from source\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    zlib1g-dev \\\n    libncurses5-dev \\\n    libgdbm-dev \\\n    libnss3-dev \\\n    libssl-dev \\\n    libreadline-dev \\\n    libffi-dev \\\n    libsqlite3-dev \\\n    libbz2-dev \\\n    tk-dev \\\n    && cd /tmp \\\n    && wget https://www.python.org/ftp/python/3.13.6/Python-3.13.6.tgz \\\n    && tar -xzf Python-3.13.6.tgz \\\n    && cd Python-3.13.6 \\\n    && ./configure --enable-optimizations \\\n    && make -j$(nproc) \\\n    && make altinstall \\\n    && ln -s /usr/local/bin/python3.13 /usr/local/bin/python \\\n    && ln -s /usr/local/bin/pip3.13 /usr/local/bin/pip \\\n    && cd / \\\n    && rm -rf /tmp/Python-3.13.6*\n\n# Install Wing Pro 11 from local file\nRUN apt-get update && apt-get install -y \\\n    libxcb-xinerama0 \\\n    libxcb-cursor0 \\\n    libqt5x11extras5 \\\n    libxkbcommon-x11-0 \\\n    libxcb-icccm4 \\\n    libxcb-image0 \\\n    libxcb-keysyms1 \\\n    libxcb-randr0 \\\n    libxcb-render-util0\n\nCOPY docker/wing-pro11_11.0.3.0_amd64.deb /tmp/\nRUN dpkg -i /tmp/wing-pro11_11.0.3.0_amd64.deb || apt-get install -f -y \\\n    && rm /tmp/wing-pro11_11.0.3.0_amd64.deb\n\n# Create user 'student' with password 'student'\nRUN useradd -m -s /bin/bash student \\\n    && echo 'student:student' | chpasswd \\\n    && adduser student sudo \\\n    && echo 'student ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n\n# Switch to student user\nUSER student\nWORKDIR /home/student\n\n# Create Wing license directory\nRUN mkdir -p ~/.wingpro11\n\n# Setup VNC for student\nRUN mkdir -p ~/.vnc \\\n    && echo \"student\" | vncpasswd -f > ~/.vnc/passwd \\\n    && chmod 600 ~/.vnc/passwd \\\n    && echo \"#!/bin/bash\\nxrdb $HOME/.Xresources\\nstartxfce4 &\" > ~/.vnc/xstartup \\\n    && chmod +x ~/.vnc/xstartup\n\n# Copy requirements and install Python packages\nCOPY docker/requirements.txt /tmp/requirements.txt\nRUN pip install --user -r /tmp/requirements.txt \\\n    && pip install --user openpyxl jupyter ipython\n\n# Copy organized structure\nCOPY src /home/student/interview-prep/src\nCOPY docs /home/student/interview-prep/docs\nCOPY data /home/student/interview-prep/data\n\n# Create startup script\nUSER root\nRUN echo '#!/bin/bash\\n\\\n# Start VNC server\\n\\\nsu - student -c \"vncserver :1 -geometry 1920x1080 -depth 24\"\\n\\\n# Start noVNC\\n\\\nwebsockify -D --web=/usr/share/novnc/ 6901 localhost:5901\\n\\\n# Keep container running\\n\\\ntail -f /dev/null' > /startup.sh \\\n    && chmod +x /startup.sh\n\n# Create desktop shortcuts\nUSER student\nRUN mkdir -p ~/Desktop \\\n    && echo '[Desktop Entry]\\n\\\nType=Application\\n\\\nName=Wing Pro 11\\n\\\nExec=/usr/bin/wing-pro11\\n\\\nIcon=/usr/share/pixmaps/wing-pro11.png\\n\\\nTerminal=false' > ~/Desktop/wing-pro.desktop \\\n    && chmod +x ~/Desktop/wing-pro.desktop\n\nRUN echo '[Desktop Entry]\\n\\\nType=Application\\n\\\nName=Terminal\\n\\\nExec=xfce4-terminal --working-directory=/home/student/interview-prep\\n\\\nIcon=utilities-terminal\\n\\\nTerminal=false' > ~/Desktop/terminal.desktop \\\n    && chmod +x ~/Desktop/terminal.desktop\n\n# Expose ports\nEXPOSE 5901 6901\n\n# Start services\nUSER root\nCMD [\"/startup.sh\"]\n",
  "docker/requirements.txt": "numpy==2.3.3\npandas==2.3.3\npython-dateutil==2.9.0.post0\npytz==2025.2\nsix==1.17.0\ntzdata==2025.2\nopenpyxl==3.1.2\n"
}

# Binary artifacts (base64 encoded)
BINARY_ARTIFACTS = {
  "docker/wing-pro11_11.0.3.0_amd64.deb": "VGhpcyB3b3VsZCBiZSB0aGUgV2luZyAuZGViIGZpbGUgY29udGVudAo=",
  "data/flashcards_complete.xlsx": "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"
}

# Root files (stay in repository root)
ROOT_ARTIFACTS = {
  "docker-compose.yml": "version: '3.8'\n\nservices:\n  interview-prep:\n    build:\n      context: .\n      dockerfile: docker/Dockerfile\n    image: interview-prep:latest\n    container_name: python-interview-prep\n    ports:\n      - \"5901:5901\"  # VNC port\n      - \"6901:6901\"  # noVNC web port\n    volumes:\n      # Mount Wing license\n      - C:/Users/rayse/AppData/Roaming/Wing Pro 11/license.act:/home/student/.wingpro11/license.act:ro\n      \n      # Mount local directory for persistence (optional)\n      # Uncomment to save your work between container restarts\n      # - ./work:/home/student/interview-prep/practice_work\n      # - ./notes:/home/student/interview-prep/notes\n    environment:\n      - DISPLAY=:1\n      - VNC_PASSWORD=student\n      - VNC_RESOLUTION=1920x1080\n    restart: unless-stopped\n    stdin_open: true\n    tty: true\n"
}

def analyze_existing(base_path):
    """Check what exists in current repo"""
    if not base_path.exists():
        return None
    
    structure = {'files': [], 'dirs': []}
    for item in base_path.iterdir():
        if item.is_file():
            structure['files'].append(item.name)
        elif item.is_dir():
            structure['dirs'].append(item.name)
    return structure

def migrate_to_organized(base_path):
    """Migrate files from flat to organized structure"""
    print("\n🔄 Migrating to organized structure...")
    moved = 0
    
    for target_dir, files in TARGET_STRUCTURE.items():
        if target_dir in PRESERVED_DIRS:
            continue
            
        for filename in files:
            old_path = base_path / filename
            if old_path.exists() and old_path.is_file():
                new_dir = base_path / target_dir
                new_dir.mkdir(parents=True, exist_ok=True)
                new_path = new_dir / filename
                
                print(f"  Moving {filename} → {target_dir}/")
                shutil.move(str(old_path), str(new_path))
                moved += 1
    
    return moved

def create_structure(base_path):
    """Create directory structure"""
    base_path.mkdir(parents=True, exist_ok=True)
    
    # Create all directories
    for dir_name in TARGET_STRUCTURE.keys():
        (base_path / dir_name).mkdir(parents=True, exist_ok=True)
    
    # Create preserved user dirs
    for dir_name in ['practice_work', 'mock_interviews', 'notes']:
        dir_path = base_path / dir_name
        dir_path.mkdir(parents=True, exist_ok=True)
        gitkeep = dir_path / '.gitkeep'
        if not gitkeep.exists():
            gitkeep.write_text('')

def write_files(base_path):
    """Write all files to organized locations"""
    print("\n📝 Writing files...")
    
    # Write text files to subdirectories
    for path, content in TEXT_ARTIFACTS.items():
        filepath = base_path / path
        filepath.parent.mkdir(parents=True, exist_ok=True)
        try:
            filepath.write_text(content, encoding='utf-8')
            print(f"  ✓ {path}")
        except Exception as e:
            # Handle encoding issues
            try:
                filepath.write_text(content, encoding='latin-1')
                print(f"  ✓ {path} (latin-1)")
            except:
                print(f"  ⚠ {path} (encoding issue)")
    
    # Write binary files
    for path, content in BINARY_ARTIFACTS.items():
        if content:
            filepath = base_path / path
            filepath.parent.mkdir(parents=True, exist_ok=True)
            try:
                decoded = base64.b64decode(content)
                filepath.write_bytes(decoded)
                print(f"  ✓ {path}")
            except Exception as e:
                print(f"  ⚠ {path} (decode error: {e})")
    
    # Write root files
    for filename, content in ROOT_ARTIFACTS.items():
        filepath = base_path / filename
        try:
            filepath.write_text(content, encoding='utf-8')
            print(f"  ✓ {filename} (root)")
        except:
            try:
                filepath.write_text(content, encoding='latin-1')
                print(f"  ✓ {filename} (root, latin-1)")
            except:
                print(f"  ⚠ {filename} (encoding issue)")

def check_preserved(base_path):
    """Check for preserved user work"""
    preserved = []
    for dir_name in ['practice_work', 'mock_interviews', 'notes']:
        dir_path = base_path / dir_name
        if dir_path.exists():
            files = [f for f in dir_path.iterdir() if f.name != '.gitkeep']
            if files:
                preserved.append(f"{dir_name} ({len(files)} files)")
    return preserved

def setup_local_repo(base_path):
    """Set up the local repository"""
    print("\n" + "="*60)
    print("Setting up local repository...")
    print("="*60)
    
    # Check existing structure
    existing = analyze_existing(base_path)
    
    if existing:
        print(f"\n✓ Found existing repository")
        
        # Check if migration needed
        needs_migration = any(f in existing['files'] for f in 
                             ['exercises.py', 'README.md', 'patterns_and_gotchas.py'])
        
        if needs_migration:
            moved = migrate_to_organized(base_path)
            if moved:
                print(f"  Migrated {moved} files")
    else:
        print(f"\n✓ Creating new repository")
    
    # Create structure and write files
    print("\n📁 Setting up organized structure...")
    create_structure(base_path)
    write_files(base_path)
    
    # Report preserved work
    preserved = check_preserved(base_path)
    if preserved:
        print(f"\n✓ Preserved: {', '.join(preserved)}")
    
    print("\n✅ Local repository ready!")
    print(f"\nLocation: {base_path}")
    print("\nTest locally:")
    print(f"  cd {base_path}")
    print(f"  python src/exercises.py")
    print("\nWhen ready for Docker:")
    print(f"  python setup_python_interview_prep.py --mode docker")

def build_docker(base_path):
    """Build Docker environment from local repo"""
    print("\n" + "="*60)
    print("Building Docker environment...")
    print("="*60)
    
    # Check if repo exists
    if not base_path.exists():
        print("\n❌ ERROR: Local repository not found!")
        print(f"  Expected at: {base_path}")
        print("\n  Run with --mode local first:")
        print("    python setup_python_interview_prep.py --mode local")
        return False
    
    # Check for docker-compose.yml
    compose_file = base_path / "docker-compose.yml"
    if not compose_file.exists():
        print("\n❌ ERROR: docker-compose.yml not found!")
        print("  Repository may be corrupted. Run --mode local again.")
        return False
    
    # Check Docker is running
    try:
        result = subprocess.run(['docker', '--version'], 
                              capture_output=True, text=True)
        if result.returncode != 0:
            raise Exception("Docker not found")
    except:
        print("\n❌ ERROR: Docker not running or not installed!")
        print("  Install Docker Desktop from: https://docker.com")
        return False
    
    print("\n🐳 Building Docker image...")
    print("  This will take 15-20 minutes the first time")
    
    # Change to repo directory and build
    os.chdir(base_path)
    
    try:
        # Run docker compose build
        result = subprocess.run(['docker', 'compose', 'build'],
                              capture_output=False, text=True)
        
        if result.returncode == 0:
            print("\n✅ Docker image built successfully!")
            print("\nTo run:")
            print(f"  cd {base_path}")
            print("  docker compose up")
            print("  Open browser to: http://localhost:6901")
            print("  Password: student")
            return True
        else:
            print("\n❌ Docker build failed!")
            print("  Check error messages above")
            return False
            
    except Exception as e:
        print(f"\n❌ Error building Docker: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(
        description='Setup Python Analytics Interview Prep',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python setup_python_interview_prep.py --mode local   # Set up repo only
  python setup_python_interview_prep.py --mode docker  # Build Docker only  
  python setup_python_interview_prep.py --mode all     # Both
        """
    )
    
    parser.add_argument(
        '--mode',
        choices=['local', 'docker', 'all'],
        default='local',
        help='Setup mode: local (repo only), docker (Docker only), all (both)'
    )
    
    args = parser.parse_args()
    
    print("="*60)
    print("Python Interview Prep - Smart Setup v3.2")
    print(f"Mode: {args.mode.upper()}")
    print(f"Configuration: {'With Wing Pro' if WING_INCLUDED else 'With JupyterLab'}")
    print("="*60)
    
    # Determine base path
    current_dir = Path.cwd()
    base_path = current_dir / REPO_NAME
    
    print(f"\nWorking directory: {current_dir}")
    
    # Execute based on mode
    if args.mode in ['local', 'all']:
        setup_local_repo(base_path)
    
    if args.mode == 'docker':
        success = build_docker(base_path)
        if not success:
            sys.exit(1)
    
    if args.mode == 'all':
        print("\n" + "="*60)
        print("Step 2: Docker Build")
        print("="*60)
        success = build_docker(base_path)
        if not success:
            print("\n⚠ Local repo is ready but Docker build failed")
            print("  Fix Docker issues and run: --mode docker")
            sys.exit(1)
    
    print("\n" + "="*60)
    print("✅ Complete!")
    print("="*60)

if __name__ == "__main__":
    main()
